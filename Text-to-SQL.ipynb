{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f050fba-799e-4283-938d-3ad980309e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colorama PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106017a-aa2b-45a9-bbf5-13be4a51a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "from colorama import Fore, Style\n",
    "import fitz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Image,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "    \"multimodalembedding@001\"\n",
    ")\n",
    "\n",
    "\n",
    "# Functions for getting text and image embeddings\n",
    "\n",
    "\n",
    "def get_text_embedding_from_text_embedding_model(\n",
    "    text: str,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates a numerical text embedding from a provided text input using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string to be embedded.\n",
    "        return_array: If True, returns the embedding as a NumPy array.\n",
    "                      If False, returns the embedding as a list. (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        list or numpy.ndarray: A 768-dimensional vector representation of the input text.\n",
    "                               The format (list or NumPy array) depends on the\n",
    "                               value of the 'return_array' parameter.\n",
    "    \"\"\"\n",
    "    embeddings = text_embedding_model.get_embeddings([text])\n",
    "    text_embedding = [embedding.values for embedding in embeddings][0]\n",
    "\n",
    "    if return_array:\n",
    "        return np.fromiter(text_embedding, dtype=float)\n",
    "\n",
    "    # returns 768 dimensional array\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def get_image_embedding_from_multimodal_embedding_model(\n",
    "    image_uri: str,\n",
    "    embedding_size: int = 512,\n",
    "    text: Optional[str] = None,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"Extracts an image embedding from a multimodal embedding model.\n",
    "    The function can optionally utilize contextual text to refine the embedding.\n",
    "\n",
    "    Args:\n",
    "        image_uri (str): The URI (Uniform Resource Identifier) of the image to process.\n",
    "        text (Optional[str]): Optional contextual text to guide the embedding generation. Defaults to \"\".\n",
    "        embedding_size (int): The desired dimensionality of the output embedding. Defaults to 512.\n",
    "        return_array (Optional[bool]): If True, returns the embedding as a NumPy array.\n",
    "        Otherwise, returns a list. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the image embedding values. If `return_array` is True, returns a NumPy array instead.\n",
    "    \"\"\"\n",
    "    image = vision_model_Image.load_from_file(image_uri)\n",
    "    embeddings = multimodal_embedding_model.get_embeddings(\n",
    "        image=image, contextual_text=text, dimension=embedding_size\n",
    "    )  # 128, 256, 512, 1408\n",
    "\n",
    "    if return_array:\n",
    "        return np.fromiter(embeddings.image_embedding, dtype=float)\n",
    "\n",
    "    return embeddings.image_embedding\n",
    "\n",
    "\n",
    "def get_text_overlapping_chunk(\n",
    "    text: str, character_limit: int = 1000, overlap: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    * Breaks a text document into chunks of a specified size, with an overlap between chunks to preserve context.\n",
    "    * Takes a text document, character limit per chunk, and overlap between chunks as input.\n",
    "    * Returns a dictionary where the keys are chunk numbers and the values are the corresponding text chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The text document to be chunked.\n",
    "        character_limit: Maximum characters per chunk (defaults to 1000).\n",
    "        overlap: Number of overlapping characters between chunks (defaults to 100).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are chunk numbers and values are the corresponding text chunks.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `overlap` is greater than `character_limit`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if overlap > character_limit:\n",
    "        raise ValueError(\"Overlap cannot be larger than character limit.\")\n",
    "\n",
    "    # Initialize variables\n",
    "    chunk_number = 1\n",
    "    chunked_text_dict = {}\n",
    "\n",
    "    # Iterate over text with the given limit and overlap\n",
    "    for i in range(0, len(text), character_limit - overlap):\n",
    "        end_index = min(i + character_limit, len(text))\n",
    "        chunk = text[i:end_index]\n",
    "\n",
    "        # Encode and decode for consistent encoding\n",
    "        chunked_text_dict[chunk_number] = chunk.encode(\"ascii\", \"ignore\").decode(\n",
    "            \"utf-8\", \"ignore\"\n",
    "        )\n",
    "\n",
    "        # Increment chunk number\n",
    "        chunk_number += 1\n",
    "\n",
    "    return chunked_text_dict\n",
    "\n",
    "\n",
    "def get_page_text_embedding(text_data: Union[dict, str]) -> dict:\n",
    "    \"\"\"\n",
    "    * Generates embeddings for each text chunk using a specified embedding model.\n",
    "    * Takes a dictionary of text chunks and an embedding size as input.\n",
    "    * Returns a dictionary where the keys are chunk numbers and the values are the corresponding embeddings.\n",
    "\n",
    "    Args:\n",
    "        text_data: Either a dictionary of pre-chunked text or the entire page text.\n",
    "        embedding_size: Size of the embedding vector (defaults to 128).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are chunk numbers or \"text_embedding\" and values are the corresponding embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    if not text_data:\n",
    "        return embeddings_dict\n",
    "\n",
    "    if isinstance(text_data, dict):\n",
    "        # Process each chunk\n",
    "        for chunk_number, chunk_value in text_data.items():\n",
    "            embeddings_dict[\n",
    "                chunk_number\n",
    "            ] = get_text_embedding_from_text_embedding_model(text=chunk_value)\n",
    "    else:\n",
    "        # Process the first 1000 characters of the page text\n",
    "        embeddings_dict[\n",
    "            \"text_embedding\"\n",
    "        ] = get_text_embedding_from_text_embedding_model(text=text_data)\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def get_chunk_text_metadata(\n",
    "    page: fitz.Page,\n",
    "    character_limit: int = 1000,\n",
    "    overlap: int = 100,\n",
    "    embedding_size: int = 128,\n",
    ") -> tuple[str, dict, dict, dict]:\n",
    "    \"\"\"\n",
    "    * Extracts text from a given page object, chunks it, and generates embeddings for each chunk.\n",
    "    * Takes a page object, character limit per chunk, overlap between chunks, and embedding size as input.\n",
    "    * Returns the extracted text, the chunked text dictionary, and the chunk embeddings dictionary.\n",
    "\n",
    "    Args:\n",
    "        page: The fitz.Page object to process.\n",
    "        character_limit: Maximum characters per chunk (defaults to 1000).\n",
    "        overlap: Number of overlapping characters between chunks (defaults to 100).\n",
    "        embedding_size: Size of the embedding vector (defaults to 128).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - Extracted page text as a string.\n",
    "            - Dictionary of embeddings for the entire page text (key=\"text_embedding\").\n",
    "            - Dictionary of chunked text (key=chunk number, value=text chunk).\n",
    "            - Dictionary of embeddings for each chunk (key=chunk number, value=embedding).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `overlap` is greater than `character_limit`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if overlap > character_limit:\n",
    "        raise ValueError(\"Overlap cannot be larger than character limit.\")\n",
    "\n",
    "    # Extract text from the page\n",
    "    text: str = page.get_text().encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "    # Get whole-page text embeddings\n",
    "    page_text_embeddings_dict: dict = get_page_text_embedding(text)\n",
    "\n",
    "    # Chunk the text with the given limit and overlap\n",
    "    chunked_text_dict: dict = get_text_overlapping_chunk(text, character_limit, overlap)\n",
    "\n",
    "    # Get embeddings for the chunks\n",
    "    chunk_embeddings_dict: dict = get_page_text_embedding(chunked_text_dict)\n",
    "\n",
    "    # Return all extracted data\n",
    "    return text, page_text_embeddings_dict, chunked_text_dict, chunk_embeddings_dict\n",
    "\n",
    "\n",
    "def get_image_for_gemini(\n",
    "    doc: fitz.Document,\n",
    "    image: tuple,\n",
    "    image_no: int,\n",
    "    image_save_dir: str,\n",
    "    file_name: str,\n",
    "    page_num: int,\n",
    ") -> Tuple[Image, str]:\n",
    "    \"\"\"\n",
    "    Extracts an image from a PDF document, converts it to JPEG format, saves it to a specified directory,\n",
    "    and loads it as a PIL Image Object.\n",
    "\n",
    "    Parameters:\n",
    "    - doc (fitz.Document): The PDF document from which the image is extracted.\n",
    "    - image (tuple): A tuple containing image information.\n",
    "    - image_no (int): The image number for naming purposes.\n",
    "    - image_save_dir (str): The directory where the image will be saved.\n",
    "    - file_name (str): The base name for the image file.\n",
    "    - page_num (int): The page number from which the image is extracted.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[Image.Image, str]: A tuple containing the Gemini Image object and the image filename.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the image from the document\n",
    "    xref = image[0]\n",
    "    pix = fitz.Pixmap(doc, xref)\n",
    "\n",
    "    # Convert the image to JPEG format\n",
    "    pix.tobytes(\"jpeg\")\n",
    "\n",
    "    # Create the image file name\n",
    "    image_name = f\"{image_save_dir}/{file_name}_image_{page_num}_{image_no}_{xref}.jpeg\"\n",
    "\n",
    "    # Create the image save directory if it doesn't exist\n",
    "    os.makedirs(image_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the image to the specified location\n",
    "    pix.save(image_name)\n",
    "\n",
    "    # Load the saved image as a Gemini Image Object\n",
    "    image_for_gemini = Image.load_from_file(image_name)\n",
    "\n",
    "    return image_for_gemini, image_name\n",
    "\n",
    "\n",
    "def get_gemini_response(\n",
    "    generative_multimodal_model,\n",
    "    model_input: List[str],\n",
    "    stream: bool = True,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This function generates text in response to a list of model inputs.\n",
    "\n",
    "    Args:\n",
    "        model_input: A list of strings representing the inputs to the model.\n",
    "        stream: Whether to generate the response in a streaming fashion (returning chunks of text at a time) or all at once. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        The generated text as a string.\n",
    "    \"\"\"\n",
    "    response = generative_multimodal_model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        stream=stream,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    response_list = []\n",
    "\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            response_list.append(chunk.text)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Exception occurred while calling gemini. Something is wrong. Lower the safety thresholds [safety_settings: BLOCK_NONE ] if not already done. -----\",\n",
    "                e,\n",
    "            )\n",
    "            response_list.append(\"Exception occurred\")\n",
    "            continue\n",
    "    response = \"\".join(response_list)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_text_metadata_df(\n",
    "    filename: str, text_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a filename and a text metadata dictionary as input,\n",
    "    iterates over the text metadata dictionary and extracts the text, chunk text,\n",
    "    and chunk embeddings for each page, creates a Pandas DataFrame with the\n",
    "    extracted data, and returns it.\n",
    "\n",
    "    Args:\n",
    "        filename: The filename of the document.\n",
    "        text_metadata: A dictionary containing the text metadata for each page.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the extracted text, chunk text, and chunk embeddings for each page.\n",
    "    \"\"\"\n",
    "\n",
    "    final_data_text: List[Dict] = []\n",
    "\n",
    "    for key, values in text_metadata.items():\n",
    "        for chunk_number, chunk_text in values[\"chunked_text_dict\"].items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"text\"] = values[\"text\"]\n",
    "            data[\"text_embedding_page\"] = values[\"page_text_embeddings\"][\n",
    "                \"text_embedding\"\n",
    "            ]\n",
    "            data[\"chunk_number\"] = chunk_number\n",
    "            data[\"chunk_text\"] = chunk_text\n",
    "            data[\"text_embedding_chunk\"] = values[\"chunk_embeddings_dict\"][chunk_number]\n",
    "\n",
    "            final_data_text.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_text)\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_image_metadata_df(\n",
    "    filename: str, image_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a filename and an image metadata dictionary as input,\n",
    "    iterates over the image metadata dictionary and extracts the image path,\n",
    "    image description, and image embeddings for each image, creates a Pandas\n",
    "    DataFrame with the extracted data, and returns it.\n",
    "\n",
    "    Args:\n",
    "        filename: The filename of the document.\n",
    "        image_metadata: A dictionary containing the image metadata for each page.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the extracted image path, image description, and image embeddings for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    final_data_image: List[Dict] = []\n",
    "    for key, values in image_metadata.items():\n",
    "        for _, image_values in values.items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"img_num\"] = int(image_values[\"img_num\"])\n",
    "            data[\"img_path\"] = image_values[\"img_path\"]\n",
    "            data[\"img_desc\"] = image_values[\"img_desc\"]\n",
    "            # data[\"mm_embedding_from_text_desc_and_img\"] = image_values[\n",
    "            #     \"mm_embedding_from_text_desc_and_img\"\n",
    "            # ]\n",
    "            data[\"mm_embedding_from_img_only\"] = image_values[\n",
    "                \"mm_embedding_from_img_only\"\n",
    "            ]\n",
    "            data[\"text_embedding_from_image_description\"] = image_values[\n",
    "                \"text_embedding_from_image_description\"\n",
    "            ]\n",
    "            final_data_image.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_image).dropna()\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_document_metadata(\n",
    "    generative_multimodal_model,\n",
    "    pdf_folder_path: str,\n",
    "    image_save_dir: str,\n",
    "    image_description_prompt: str,\n",
    "    embedding_size: int = 128,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    add_sleep_after_page: bool = False,\n",
    "    sleep_time_after_page: int = 2,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function takes a PDF path, an image save directory, an image description prompt, an embedding size, and a text embedding text limit as input.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The path to the PDF document.\n",
    "        image_save_dir: The directory where extracted images should be saved.\n",
    "        image_description_prompt: A prompt to guide Gemini for generating image descriptions.\n",
    "        embedding_size: The dimensionality of the embedding vectors.\n",
    "        text_emb_text_limit: The maximum number of tokens for text embedding.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two DataFrames:\n",
    "            * One DataFrame containing the extracted text metadata for each page of the PDF, including the page text, chunked text dictionaries, and chunk embedding dictionaries.\n",
    "            * Another DataFrame containing the extracted image metadata for each image in the PDF, including the image path, image description, image embeddings (with and without context), and image description text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    text_metadata_df_final, image_metadata_df_final = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    for pdf_path in glob.glob(pdf_folder_path + \"/*.pdf\"):\n",
    "        print(\n",
    "            \"\\n\\n\",\n",
    "            \"Processing the file: ---------------------------------\",\n",
    "            pdf_path,\n",
    "            \"\\n\\n\",\n",
    "        )\n",
    "\n",
    "        # Open the PDF file\n",
    "        doc: fitz.Document = fitz.open(pdf_path)\n",
    "\n",
    "        file_name = pdf_path.split(\"/\")[-1]\n",
    "\n",
    "        text_metadata: Dict[Union[int, str], Dict] = {}\n",
    "        image_metadata: Dict[Union[int, str], Dict] = {}\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            print(f\"Processing page: {page_num + 1}\")\n",
    "\n",
    "            text = page.get_text()\n",
    "            (\n",
    "                text,\n",
    "                page_text_embeddings_dict,\n",
    "                chunked_text_dict,\n",
    "                chunk_embeddings_dict,\n",
    "            ) = get_chunk_text_metadata(page, embedding_size=embedding_size)\n",
    "\n",
    "            text_metadata[page_num] = {\n",
    "                \"text\": text,\n",
    "                \"page_text_embeddings\": page_text_embeddings_dict,\n",
    "                \"chunked_text_dict\": chunked_text_dict,\n",
    "                \"chunk_embeddings_dict\": chunk_embeddings_dict,\n",
    "            }\n",
    "\n",
    "            images = page.get_images()\n",
    "            image_metadata[page_num] = {}\n",
    "\n",
    "            for image_no, image in enumerate(images):\n",
    "                image_number = int(image_no + 1)\n",
    "                image_metadata[page_num][image_number] = {}\n",
    "\n",
    "                image_for_gemini, image_name = get_image_for_gemini(\n",
    "                    doc, image, image_no, image_save_dir, file_name, page_num\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Extracting image from page: {page_num + 1}, saved as: {image_name}\"\n",
    "                )\n",
    "\n",
    "                response = get_gemini_response(\n",
    "                    generative_multimodal_model,\n",
    "                    model_input=[image_description_prompt, image_for_gemini],\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
    "                    image_uri=image_name,\n",
    "                    embedding_size=embedding_size,\n",
    "                )\n",
    "\n",
    "                image_description_text_embedding = (\n",
    "                    get_text_embedding_from_text_embedding_model(text=response)\n",
    "                )\n",
    "\n",
    "                image_metadata[page_num][image_number] = {\n",
    "                    \"img_num\": image_number,\n",
    "                    \"img_path\": image_name,\n",
    "                    \"img_desc\": response,\n",
    "                    # \"mm_embedding_from_text_desc_and_img\": image_embedding_with_description,\n",
    "                    \"mm_embedding_from_img_only\": image_embedding,\n",
    "                    \"text_embedding_from_image_description\": image_description_text_embedding,\n",
    "                }\n",
    "\n",
    "            # Add sleep to reduce issues with Quota error on API\n",
    "            if add_sleep_after_page:\n",
    "                time.sleep(sleep_time_after_page)\n",
    "                print(\n",
    "                    \"Sleeping for \",\n",
    "                    sleep_time_after_page,\n",
    "                    \"\"\" sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \"\"\",\n",
    "                )\n",
    "\n",
    "        text_metadata_df = get_text_metadata_df(file_name, text_metadata)\n",
    "        image_metadata_df = get_image_metadata_df(file_name, image_metadata)\n",
    "\n",
    "        text_metadata_df_final = pd.concat(\n",
    "            [text_metadata_df_final, text_metadata_df], axis=0\n",
    "        )\n",
    "        image_metadata_df_final = pd.concat(\n",
    "            [\n",
    "                image_metadata_df_final,\n",
    "                image_metadata_df.drop_duplicates(subset=[\"img_desc\"]),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        text_metadata_df_final = text_metadata_df_final.reset_index(drop=True)\n",
    "        image_metadata_df_final = image_metadata_df_final.reset_index(drop=True)\n",
    "\n",
    "    return text_metadata_df_final, image_metadata_df_final\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "\n",
    "def get_user_query_text_embeddings(user_query: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts text embeddings for the user query using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        user_query: The user query text.\n",
    "        embedding_size: The desired embedding size.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the user query text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_text_embedding_from_text_embedding_model(user_query)\n",
    "\n",
    "\n",
    "def get_user_query_image_embeddings(\n",
    "    image_query_path: str, embedding_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts image embeddings for the user query image using a multimodal embedding model.\n",
    "\n",
    "    Args:\n",
    "        image_query_path: The path to the user query image.\n",
    "        embedding_size: The desired embedding size.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the user query image embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_image_embedding_from_multimodal_embedding_model(\n",
    "        image_uri=image_query_path, embedding_size=embedding_size\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cosine_score(\n",
    "    dataframe: pd.DataFrame, column_name: str, input_text_embed: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between the user query embedding and the dataframe embedding for a specific column.\n",
    "\n",
    "    Args:\n",
    "        dataframe: The pandas DataFrame containing the data to compare against.\n",
    "        column_name: The name of the column containing the embeddings to compare with.\n",
    "        input_text_embed: The NumPy array representing the user query embedding.\n",
    "\n",
    "    Returns:\n",
    "        The cosine similarity score (rounded to two decimal places) between the user query embedding and the dataframe embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    return round(np.dot(dataframe[column_name], input_text_embed), 2)\n",
    "\n",
    "\n",
    "def print_text_to_image_citation(\n",
    "    final_images: Dict[int, Dict[str, Any]], print_top: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted citation for each matched image in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        final_images: A dictionary containing information about matched images,\n",
    "                    with keys as image number and values as dictionaries containing\n",
    "                    image path, page number, page text, cosine similarity score, and image description.\n",
    "        print_top: A boolean flag indicating whether to only print the first citation (True) or all citations (False).\n",
    "\n",
    "    Returns:\n",
    "        None (prints formatted citations to the console).\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through the matched image citations\n",
    "    for imageno, image_dict in final_images.items():\n",
    "        # Print the citation header\n",
    "        print(f\"{Fore.RED}Citation {imageno + 1}:{Style.RESET_ALL}\")\n",
    "        print(\"Matched image path, page number, and page text:\")\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(f\"{Fore.BLUE}Score:{Style.RESET_ALL}\", image_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(f\"{Fore.BLUE}File name:{Style.RESET_ALL}\", image_dict[\"file_name\"])\n",
    "\n",
    "        # Print the image path\n",
    "        print(f\"{Fore.BLUE}Path:{Style.RESET_ALL}\", image_dict[\"img_path\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(f\"{Fore.BLUE}Page number:{Style.RESET_ALL}\", image_dict[\"page_num\"])\n",
    "\n",
    "        # Print the page text\n",
    "        print(\n",
    "            f\"{Fore.BLUE}Page text:{Style.RESET_ALL}\",\n",
    "            \"\\n\".join(image_dict[\"page_text\"]),\n",
    "        )\n",
    "\n",
    "        # Print the image description\n",
    "        print(\n",
    "            f\"{Fore.BLUE}Image description:{Style.RESET_ALL}\",\n",
    "            image_dict[\"image_description\"],\n",
    "        )\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and imageno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "def print_text_to_text_citation(\n",
    "    final_text: Dict[int, Dict[str, Any]],\n",
    "    print_top: bool = True,\n",
    "    chunk_text: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted citation for each matched text in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        final_text: A dictionary containing information about matched text passages,\n",
    "                    with keys as text number and values as dictionaries containing\n",
    "                    page number, cosine similarity score, chunk number (optional),\n",
    "                    chunk text (optional), and page text (optional).\n",
    "        print_top: A boolean flag indicating whether to only print the first citation (True) or all citations (False).\n",
    "        chunk_text: A boolean flag indicating whether to print individual text chunks (True) or the entire page text (False).\n",
    "\n",
    "    Returns:\n",
    "        None (prints formatted citations to the console).\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through the matched text citations\n",
    "    for textno, text_dict in final_text.items():\n",
    "        # Print the citation header\n",
    "        print(f\"{Fore.RED}Citation {textno + 1}: Matched text:{Style.RESET_ALL}\")\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(f\"{Fore.BLUE}Score:{Style.RESET_ALL}\", text_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(f\"{Fore.BLUE}File name:{Style.RESET_ALL}\", text_dict[\"file_name\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(f\"{Fore.BLUE}Page:{Style.RESET_ALL}\", text_dict[\"page_num\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(f\"{Fore.BLUE}Page number:{Style.RESET_ALL}\", text_dict[\"page_num\"])\n",
    "\n",
    "        # Print the matched text based on the chunk_text argument\n",
    "        if chunk_text:\n",
    "            # Print chunk number and chunk text\n",
    "            print(\n",
    "                f\"{Fore.BLUE}Chunk number:{Style.RESET_ALL}\", text_dict[\"chunk_number\"]\n",
    "            )\n",
    "            print(f\"{Fore.BLUE}Chunk text:{Style.RESET_ALL}\", text_dict[\"chunk_text\"])\n",
    "        else:\n",
    "            # Print page text\n",
    "            print(f\"{Fore.BLUE}Page text:{Style.RESET_ALL}\", text_dict[\"page_text\"])\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and textno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_similar_image_from_query(\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    image_metadata_df: pd.DataFrame,\n",
    "    query: str = \"\",\n",
    "    image_query_path: str = \"\",\n",
    "    column_name: str = \"\",\n",
    "    image_emb: bool = True,\n",
    "    top_n: int = 3,\n",
    "    embedding_size: int = 128,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the top N most similar images from a metadata DataFrame based on a text query or an image query.\n",
    "\n",
    "    Args:\n",
    "        text_metadata_df: A Pandas DataFrame containing text metadata associated with the images.\n",
    "        image_metadata_df: A Pandas DataFrame containing image metadata (paths, descriptions, etc.).\n",
    "        query: The text query used for finding similar images (if image_emb is False).\n",
    "        image_query_path: The path to the image used for finding similar images (if image_emb is True).\n",
    "        column_name: The column name in the image_metadata_df containing the image embeddings or captions.\n",
    "        image_emb: Whether to use image embeddings (True) or text captions (False) for comparisons.\n",
    "        top_n: The number of most similar images to return.\n",
    "        embedding_size: The dimensionality of the image embeddings (only used if image_emb is True).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing information about the top N most similar images, including cosine scores, image objects, paths, page numbers, text excerpts, and descriptions.\n",
    "    \"\"\"\n",
    "    # Check if image embedding is used\n",
    "    if image_emb:\n",
    "        # Calculate cosine similarity between query image and metadata images\n",
    "        user_query_image_embedding = get_user_query_image_embeddings(\n",
    "            image_query_path, embedding_size\n",
    "        )\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_image_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "    else:\n",
    "        # Calculate cosine similarity between query text and metadata image captions\n",
    "        user_query_text_embedding = get_user_query_text_embeddings(query)\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_text_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Remove same image comparison score when user image is matched exactly with metadata image\n",
    "    cosine_scores = cosine_scores[cosine_scores < 1.0]\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_cosine_scores = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_cosine_values = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched images and their information\n",
    "    final_images: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_imageno, indexvalue in enumerate(top_n_cosine_scores):\n",
    "        # Create a sub-dictionary for each matched image\n",
    "        final_images[matched_imageno] = {}\n",
    "\n",
    "        # Store cosine score\n",
    "        final_images[matched_imageno][\"cosine_score\"] = top_n_cosine_values[\n",
    "            matched_imageno\n",
    "        ]\n",
    "\n",
    "        # Load image from file\n",
    "        final_images[matched_imageno][\"image_object\"] = Image.load_from_file(\n",
    "            image_metadata_df.iloc[indexvalue][\"img_path\"]\n",
    "        )\n",
    "\n",
    "        # Add file name\n",
    "        final_images[matched_imageno][\"file_name\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"file_name\"\n",
    "        ]\n",
    "\n",
    "        # Store image path\n",
    "        final_images[matched_imageno][\"img_path\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"img_path\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_images[matched_imageno][\"page_num\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"page_num\"\n",
    "        ]\n",
    "\n",
    "        final_images[matched_imageno][\"page_text\"] = np.unique(\n",
    "            text_metadata_df[\n",
    "                (\n",
    "                    text_metadata_df[\"page_num\"].isin(\n",
    "                        [final_images[matched_imageno][\"page_num\"]]\n",
    "                    )\n",
    "                )\n",
    "                & (\n",
    "                    text_metadata_df[\"file_name\"].isin(\n",
    "                        [final_images[matched_imageno][\"file_name\"]]\n",
    "                    )\n",
    "                )\n",
    "            ][\"text\"].values\n",
    "        )\n",
    "\n",
    "        # Store image description\n",
    "        final_images[matched_imageno][\"image_description\"] = image_metadata_df.iloc[\n",
    "            indexvalue\n",
    "        ][\"img_desc\"]\n",
    "\n",
    "    return final_images\n",
    "\n",
    "\n",
    "def get_similar_text_from_query(\n",
    "    query: str,\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    column_name: str = \"\",\n",
    "    top_n: int = 3,\n",
    "    chunk_text: bool = True,\n",
    "    print_citation: bool = False,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds the top N most similar text passages from a metadata DataFrame based on a text query.\n",
    "\n",
    "    Args:\n",
    "        query: The text query used for finding similar passages.\n",
    "        text_metadata_df: A Pandas DataFrame containing the text metadata to search.\n",
    "        column_name: The column name in the text_metadata_df containing the text embeddings or text itself.\n",
    "        top_n: The number of most similar text passages to return.\n",
    "        embedding_size: The dimensionality of the text embeddings (only used if text embeddings are stored in the column specified by `column_name`).\n",
    "        chunk_text: Whether to return individual text chunks (True) or the entire page text (False).\n",
    "        print_citation: Whether to immediately print formatted citations for the matched text passages (True) or just return the dictionary (False).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing information about the top N most similar text passages, including cosine scores, page numbers, chunk numbers (optional), and chunk text or page text (depending on `chunk_text`).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified `column_name` is not present in the `text_metadata_df`.\n",
    "    \"\"\"\n",
    "\n",
    "    if column_name not in text_metadata_df.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' not found in the 'text_metadata_df'\")\n",
    "\n",
    "    query_vector = get_user_query_text_embeddings(query)\n",
    "\n",
    "    # Calculate cosine similarity between query text and metadata text\n",
    "    cosine_scores = text_metadata_df.apply(\n",
    "        lambda row: get_cosine_score(\n",
    "            row,\n",
    "            column_name,\n",
    "            query_vector,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_indices = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_scores = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched text and their information\n",
    "    final_text: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_textno, index in enumerate(top_n_indices):\n",
    "        # Create a sub-dictionary for each matched text\n",
    "        final_text[matched_textno] = {}\n",
    "\n",
    "        # Store description\n",
    "        final_text[matched_textno][\"description\"] = text_metadata_df.iloc[index][\n",
    "            \"description\"\n",
    "        ]\n",
    "\n",
    "        # Store data_type\n",
    "        final_text[matched_textno][\"data_type\"] = text_metadata_df.iloc[index][\n",
    "            \"data_type\"\n",
    "        ]\n",
    "\n",
    "        final_text[matched_textno][\"table_name\"] = text_metadata_df.iloc[index][\n",
    "            \"table_name\"\n",
    "        ]\n",
    "\n",
    "        # Store cosine score\n",
    "        final_text[matched_textno][\"cosine_score\"] = top_n_scores[matched_textno]\n",
    "\n",
    "        if chunk_text:\n",
    "            # Store chunk number\n",
    "            final_text[matched_textno][\"chunk_number\"] = text_metadata_df.iloc[index][\n",
    "                \"chunk_number\"\n",
    "            ]\n",
    "\n",
    "            # Store chunk text\n",
    "            final_text[matched_textno][\"chunk_text\"] = text_metadata_df[\"chunk_text\"][\n",
    "                index\n",
    "            ]\n",
    "        else:\n",
    "            # Store column_name\n",
    "            final_text[matched_textno][\"column_name\"] = text_metadata_df[\"column_name\"][index]\n",
    "\n",
    "    # Optionally print citations immediately\n",
    "    if print_citation:\n",
    "        print_text_to_text_citation(final_text, chunk_text=chunk_text)\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: Iterable[Union[str, PIL.Image.Image]], resize_ratio: float = 0.5\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a series of images provided as paths or PIL Image objects.\n",
    "\n",
    "    Args:\n",
    "        images: An iterable of image paths or PIL Image objects.\n",
    "        resize_ratio: The factor by which to resize each image (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "        None (displays images using IPython or Jupyter notebook).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert paths to PIL images if necessary\n",
    "    pil_images = []\n",
    "    for image in images:\n",
    "        if isinstance(image, str):\n",
    "            pil_images.append(PIL.Image.open(image))\n",
    "        else:\n",
    "            pil_images.append(image)\n",
    "\n",
    "    # Resize and display each image\n",
    "    for img in pil_images:\n",
    "        original_width, original_height = img.size\n",
    "        new_width = int(original_width * resize_ratio)\n",
    "        new_height = int(original_height * resize_ratio)\n",
    "        resized_img = img.resize((new_width, new_height))\n",
    "        display(resized_img)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acd495-f07e-48d2-a6e8-3da44dfa765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "PROJECT_ID = \"prj-ot-dev-analyticspg-8a50\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel\n",
    "\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39360212-410b-4966-bfe4-de929839f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "text_model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision-001\")\n",
    "\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "    \"multimodalembedding@001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce3961-e24a-4080-9496-e520e974eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ea9d7-f870-4525-b717-ec9c4e0d0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import re\n",
    "\n",
    "# Read the parquet file\n",
    "embeddings_DataDictionary = pd.read_parquet(\"embeddings_DataDictionary.pq\")\n",
    "\n",
    "# Query for the question\n",
    "query = \"\"\"\\\n",
    "Question:\n",
    " -  Which customer had the highest volume of shipments last month?\n",
    "\"\"\"\n",
    "\n",
    "# Get matching results chunks data\n",
    "\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    embeddings_DataDictionary,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=10,\n",
    "    chunk_text=False,\n",
    ")\n",
    "\n",
    "context_text = []\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.append(value[\"column_name\"] + \" \" + value[\"data_type\"] + \" \" + value[\"description\"] + \" \" + value[\"table_name\"])\n",
    "final_context_text = \"\\n\".join(context_text)\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Instructions: You are an AI assistant designed to generate SQL queries for the Onelook BigQuery database.\n",
    "Write a SQL Query. Write it in such a way that this SQL can be further executed in BigQuery.\n",
    "If no limit or offset is mentioned and if the data is expected to be large, use 0 for the offset and 10 for the limit as default. User will give in the question if they need a specific page.\n",
    "Use the schema namespace `prj-ot-dev-bqsandbox-000001.onelook_test` as default for all tables when writing the SQL Query. Use joins if needed.\n",
    "\n",
    "You have access to the following tables:\n",
    "\n",
    "\n",
    "dim_eventtype\n",
    "The table lists and categorizes event types used to track and manage shipment statuses and processes within the logistics network. The table includes unique event identifiers, labels, and status indicators.It also links events to their respective systems and processes.\n",
    "column_name | data_type | description\n",
    "EventTypeKey\t|    INT64\t|\tUnique identifier for the type of event\n",
    "SourceSystemKey\t|    INT64\t|\tUnique identifier for the source system of the event\n",
    "EventType\t|    STRING\t|\tDescribes the type of event\n",
    "EventModifier\t|    STRING\t|\tIn combination with EventType, describes the type of the event\n",
    "LegacyEventTypeId\t|    STRING\t|\tIdentifier used in a previous system for the event type.\n",
    "LegacyEventModifierId\t|    STRING\t|\tIdentifier used in a previous system for the event modifier.\n",
    "EventCategory\t|    STRING\t|\tCategory classification of the event.\n",
    "Label\t|    STRING\t|\tLabel or name assigned to the event type.\n",
    "ShortText\t|    STRING\t|\tBrief description of the event type.\n",
    "LongText\t|    STRING\t|\tDetailed description of the event type\n",
    "PublicLabel\t|    STRING\t|\tLabel intended for public display.\n",
    "PublicShortText\t|    STRING\t|\tBrief description intended for public display.\n",
    "PublicLongText\t|    STRING\t|\tDetailed description intended for public display.\n",
    "Controllable\t|    STRING\t|\tIndicator of whether the event type is controllable\n",
    "IsCompletionEvent\t|    BOOL\t|\tflag indicating if the event is a completion event.\n",
    "IncludeInFactPackageProcess\t|    BOOL\t|    Boolean flag indicating if the event is included in the fact package process.\n",
    "IsFacilityTouch\t|    BOOL\t|    Boolean flag indicating if the event involves a facility touch.\n",
    "\n",
    "\n",
    "dim_customerfacility\n",
    "The table captures details about customer-specific facilities, including facility identifiers and geographic details. It also categorizes the type of location and links facilities to their respective customers.\n",
    "column_name | data_type | description\n",
    "CustomerFacilityKey\t|    INT64\t|\tUnique identifier for the customer facility.\n",
    "CustomerId\t|    STRING\t|\tunique identifier for the customer.\n",
    "CustomerKey\t|    INT64\t|\tThe customer key associated with this facilit\n",
    "SourceSystemKey\t|    INT64\t|\tIdentifier for the source system of the data.\n",
    "CustomerFacilityId\t|    STRING\t|\tIdentifier for the customer facility in the source system.\n",
    "LocationType\t|    STRING\t|\tDescription of the type or category of the location.\n",
    "Address1\t|    STRING\t|\tPrimary address line for the customer facility.\n",
    "Address2\t|    STRING\t|\tSecondary address line for the customer facility.\n",
    "City\t|    STRING\t|\tCity where the customer facility is located\n",
    "State\t|    STRING\t|\tState or region where the customer facility is located.\n",
    "ZipCode\t|    STRING\t|\tPostal code for the customer facility address.\n",
    "Country\t|    STRING\t|\tCountry where the customer facility is located.\n",
    "InjectionFacilities\t|    STRING\t|\tDescription or identifier related to injection facilities associated with the customer.\n",
    "InjectionPostalCode\t|    STRING\t|\tPostal code for the injection facilities.\n",
    "\n",
    "\n",
    "fact_package\n",
    "This table stores detailed records of shipments and package tracking events. It includes key details such as package identifiers, dates , and status flags. The table also captures location information , transit times, and financial data.\n",
    "column_name | data_type | description\n",
    "FactPackageKey\t|    INT64\t|\tUnique ID for this record\n",
    "FactId\t|    STRING\t|\tUnique ID for this record\n",
    "FactDateKey\t|    INT64\t|\tdate key that this record was inserted into FactPackage\n",
    "SourceSystemKey\t|    INT64\t|\tunique identifier assigned to Source System\n",
    "AccountKey\t|    INT64\t|\tAccountKey associated with this package\n",
    "CustomerKey\t|    INT64\t|\tThe customer who shipped this package\n",
    "ServiceKey\t|    INT64\t|\tThe type of service the package was delivered with\n",
    "DestinationBranchKey\t|    INT64\t|\tunique identifier assigned to the Destination Branch\n",
    "CustomerFacilityKey\t|    INT64\t|\tunique identifier assigned to the Customer Facility\n",
    "RunId  |\tFLOAT64\t|\tunique identifier for the specific run\n",
    "BarcodeId\t|    STRING\t|\tIdentification number generated on the package\n",
    "OrderNumber\t|    STRING\t|\tunique identifier assigned to each transaction made\n",
    "InvoiceNumber\t|    STRING\t|\tunique identifier assigned to each invoice generated by a business.Its used for tracking, reference, and record-keeping purposes\n",
    "ManifestDateKey\t|   INT64\t|\tunique identifier assigned to the date and time when a shipping manifest was created for a package\n",
    "ManifestDatetime\t|   The date and time when a shipping manifest was created for a package\n",
    "OriginSystem\t|    STRING\t|\tOrigin or warehouse of the package\n",
    "FirstSortFacilityKey\t|    INT64\t|\tUnique identifier for theFirst sorting facility the package passed through.\n",
    "FirstSortDateKey\t|    INT64\t|\tunique identifier assigned to the earliest date if the sorting is done based on a date field\n",
    "FirstSortDateTime\t|   DATETIME\t|\tTimestamp of First Sorting\n",
    "FirstIntermediateSortFacilityKey\t|    INT64\t|\tIdentifies the facility of First Intermediate sort\n",
    "FirstIntermediateSortDateKey\t|    INT64\t|\tIdentifies the date of First Intermediate Sort\n",
    "FirstIntermediateSortDatetime  |    DATETIME\t|\tTimestamp of First Intermediate Sort\n",
    "SecondIntermediateSortFacilityKey\t|    INT64\t|\tIdentifies the facility of Second Intermediate sort\n",
    "SecondIntermediateSortDateKey\t|    INT64\t|\tIdentifies the date of Second Intermediate Sort\n",
    "SecondIntermediateSortDatetime    |    DATETIME\t|\tTimestamp of Second Intermediate Sort\n",
    "LastSortFacilityKey\t|    INT64\t|\tunique identifier assigned to the Facility of last sorting\n",
    "LastSortDateKey\t|    INT64\t|\tunique identifier assigned to date of last sorting\n",
    "LastSortDatetime   |    DATETIME\t|\tTimestamp of Last Sorting\n",
    "DestinationFacilityKey\t|    INT64\t|\tunique identifier assigned to indicate destination Facility\n",
    "DestinationDateKey\t|    INT64\t|\tunique identifier assigned to the date  when a package or shipment is expected to arrive at its final destination\n",
    "DestinationDatetime    |    DATETIME\t|\tTimestamp of the package expected to arrive at its final destination\n",
    "SortFacilityCount\t|    INT64\t|\tNumber of sorting facilities the package passed through.\n",
    "LoadDateKey\t|    INT64\t|\tunique identifier assigned to the date  when a shipment is loaded\n",
    "LoadDatetime\t|    DATETIME\t|\tthe date and time when a shipment is loaded\n",
    "DepartedDateKey\t|    INT64\t|\tunique identifier assigned to the date when a shipment departs from\n",
    "DepartedDatetime  |    DATETIME\t|\tthe date and time when a shipment departs from a facility or location.\n",
    "FirstAttemptFacilityKey\t|    INT64\t|\tunique identifier assigned to the facility where the first delivery attempt was made.\n",
    "FirstAttemptDateKey\t|    INT64\t|\tunique identifier assigned to  the date of the first delivery attempt.\n",
    "FirstAttemptDatetime\t|    DATETIME\t|\tthe date and time of the first delivery attempt.\n",
    "DeliveryFacilityKey\t|    INT64\t|\tunique identifier assigned to for the facility where the delivery was made.\n",
    "DeliveryDateKey\t|    INT64\t|\tunique identifier assigned to the date when a delivery was made.\n",
    "DeliveryDatetime |    DATETIME\t|\tthe date and time when a delivery was made.\n",
    "CustomerCommitmentDateKey\t|    INT64\t|  DateKey for when we have committed to delivering the package\n",
    "isNetworkOntime\t|    BOOL\t|\tIndicates if the package was on time according to network standards.\n",
    "isDestinationFacilityOntime\t|    BOOL\t|\tIndicates if the package will reach its destination facility on time\n",
    "isCustomerOntime\t|    BOOL\t|\tIndicates if the package was on time to meet the CustomerCommitment\n",
    "DaysLate\t|    INT64\t|\tThe number of days a delivery was late\n",
    "DaysEarly\t|    INT64\t|\tthe number of days a delivery was early\n",
    "ActualTransitDays\t|    INT64\t|\tRepresent the actual number of days a shipment was in transit (excludes holidays)\n",
    "ActualTransitDaysManifest\t|    INT64\t|\trepresent the actual number of days a shipment was in transit, as recorded in the manifest.\n",
    "TransitCategory\t|    STRING\t|\tTransit Category for the Package\n",
    "TotalDeliveryCost\tFLOAT64\t|\tThe total delivery cost for this package\n",
    "RevenueExclFuel\tFLOAT64\t|\tThe revenue generated from the delivery, excluding the cost of fuel.\n",
    "Fuel\tFLOAT64\t|\tThe cost of fuel used for the delivery.\n",
    "VolumeCount\t|    BOOL\t|\tThe count of items for this delivery\n",
    "BillableTransactionDateKey\t|    INT64\t|\tdate of the billable transaction.\n",
    "DestinationZipCode\t|    STRING\t|\tThe zip code of the delivery destination.\n",
    "OriginZipCode\t|    STRING\t|\tThe zip code where the shipment originated.\n",
    "PriceZone\t|    INT64\t|\tA classification based on pricing, likely related to shipping zones.\n",
    "hasVPOD\t|    BOOL\t|\tIndicates if a voice proof of delivery was obtained\n",
    "hasSignature\t|    BOOL\t|\tIndicates if the shipment has a proof of delivery.\n",
    "isSignatureRequired\t|    BOOL\t|\tIndicates if the shipment required a Signature\n",
    "hasManualDelivery\t|    BOOL\t|\tIndicates if the delivery was manual.\n",
    "hasScannedDelivery\t|    BOOL\t|\tIndicates if the delivery was scanned\n",
    "isShortage\t|    BOOL\t|\tIndicates if the delivery was shortage.\n",
    "isDeleted\t|    BOOL\t|\tIndicates if the package was deleted\n",
    "WeatherFlag\t|    BOOL\t|\tIndicates if the package was affected by weather.\n",
    "WeatherFlagFacilityKey\t|    INT64\t|\tIdentifies the Facility affected by weather.\n",
    "WeatherFlagDateKey\t|    INT64\t|\tIdentifies the date of Weather impact\n",
    "WeatherFlagDatetime   |    DATETIME\t|\tTimestamp of the weather impact\n",
    "LostFlag\t|    BOOL\t|\tIndicates if the shipment was lost\n",
    "LostFlagFacilityKey\t|    INT64\t|\tIdentifies the facility where the package was lost\n",
    "LostFlagDateKey\t|    INT64\t|  DATE of the package loss\n",
    "LostFlagDatetime    |    DATETIME\t|\tTimestamp of the package loss\n",
    "DamageFlag\t|    BOOL\t|\t Indicates if the package was damaged.\n",
    "DamageFlagFacilityKey\t|    INT64\t|\tIdentifies the Facility where the package was damaged.\n",
    "DamageFlagDateKey\t|    INT64\t|\tIdentifies the date of package damage.\n",
    "DamageFlagDatetime   |    DATETIME\t|\tTimestamp of the package damage\n",
    "MissortFlag\t|    BOOL\t|\tIndicates if the package was missorted.\n",
    "MissortFlagFacilityKey\t|    INT64\t|\tIdentifies the Facility where the package was missorted.\n",
    "MissortFlagDateKey\t|    INT64\t|\tIdentifies the date when the package was missorted\n",
    "MissortFlagDatetime    |    DATETIME\t|\tTimestamp of Package missort\n",
    "OnHoldFlag\t|    BOOL\t|\tindicates if the package was on missort\n",
    "OnHoldFlagFacilityKey\t|    INT64\t|\tIdentifies where the package was on hold\n",
    "OnHoldFlagDateKey\t|    INT64\t|\tIdentifies the date of package which was on hold\n",
    "OnHoldFlagDatetime    |    DATETIME\t|\tTimestamp of the package on hold\n",
    "RTSFlag\t|    BOOL\t|\tIndicated if the package was returned to sender\n",
    "RTSFlagFacilityKey\t|    INT64\t|\tIdentifies if the package was returned to sender\n",
    "RTSFlag|    DATEKey\t|    INT64\t|\tIdentifies the date of package return to sender\n",
    "RTSFlagDatetime    |    DATETIME\t|\tTimestamp of the package return\n",
    "LastExceptionOrAttempt|    DATEKey\t|    INT64\t|\tIdentifies the date of last exception or attempt occurred\n",
    "LastExceptionOrAttemptDatetime\t    |    DATETIME\t|\tTimestamp of the date of last exception or attempt occurred\n",
    "LastExceptionOrAttemptFacilityKey\t|    INT64\t|\tIdentifies the facility of last exception or attempt occurred\n",
    "LastExceptionOrAttemptEventTypeKey\t|    INT64\t|\tIdentifies the Type of the last exception or attempt.\n",
    "CurrentPackageStatusEventTypeKey\t|    INT64\t|\tIdentifies the Current status of the package.\n",
    "WestInsertedLogId\t|    INT64\t|\tIdentifier for a log entry made in the western region.\n",
    "EastInsertedLogId\t|    INT64\t|\tIdentifier for a log entry made in the eastern region.\n",
    "WestInsertedDatetime    |    DATETIME\t|\tTimestamp for when a log entry was made in the western region.\n",
    "EastInsertedDatetime    |    DATETIME\t|\tTimestamp for when a log entry was made in the eastern region.\n",
    "RunCreatedLocalDateKey\t|    INT64\t|\tThe local date when a batch of operations or events was created.\n",
    "RunCreatedLocalDatetime    |    DATETIME\t|\tThe Local timestamp when a batch of operations or events was created.\n",
    "DestinationLocationType\t|    STRING\t|\tType of the destination location of a package\n",
    "\n",
    "\n",
    "dim_branch\n",
    "This table represents branch details, including unique identifiers and names. It is linked to facilities and other location-based operations, providing a geographic and operational structure for the transportation network.\n",
    "column_name | data_type | description\n",
    "BranchKey\t|    INT64\t|\tNumeric identifier for the branch.\n",
    "SourceSystemKey\t|    INT64\t|\tNumeric key representing the source system of the branch data.\n",
    "BranchCodeId\t|    STRING\t|\tIdentifier for the branch code\n",
    "BranchName\t|    STRING\t|\tName of the branch\n",
    "\n",
    "\n",
    "dim_facility\n",
    "This table contains detailed facility information, including identifiers, geographic location , and operational details. It also tracks facility relationships and time-related information.\n",
    "column_name | data_type | description\n",
    "FacilityKey\t|    INT64\t|\tNumeric identifier for the facility.\n",
    "BranchKey\t|    INT64\t|\tNumeric key representing the branch to which the facility belongs.\n",
    "SourceSystemKey\t|    INT64\t|\tNumeric key for the source system of the facility data.\n",
    "FacilityId\t|    STRING\t|\tAlphanumeric identifier for the facility.\n",
    "WestFacility\t|    STRING\t|\tIdentifier or code for a facility in the western region.\n",
    "EastFacility\t|    STRING\t|\tIdentifier or code for a facility in the eastern region.\n",
    "BranchCode\t|    STRING\t|\tCode associated with the branch of the facility.\n",
    "LegacyCompany\t|    STRING\t|\tThe legacy company associated with the facility\n",
    "FacilityName\t|    STRING\t|\tName of the facility\n",
    "IsActive\t|    INT64\t|\tNumeric flag indicating if the facility is active (e.g., 1 for active, 0 for inactive).\n",
    "Region\t|    STRING\t|\tGeographic or operational region of the facility.\n",
    "Division\t|    STRING\t|\tDivision or sector to which the facility belongs.\n",
    "Address1\t|    STRING\t|\tPrimary address line for the facility.\n",
    "Address2\t|    STRING\t|\tSecondary address line for the facility.\n",
    "City\t|    STRING\t|\tCity where the facility is located.\n",
    "State\t|    STRING\t|\tState where the facility is located.\n",
    "ZipCode\t|    STRING\t|\tPostal code for the facility address\n",
    "Latitude\tFLOAT64\t|\tGeographic latitude coordinate of the facility\n",
    "Longitude\tFLOAT64\t|\tGeographic longitude coordinate of the facility\n",
    "RegularDeliveryCutoffTime\tTIME\t|\tTime of day for regular delivery cutoff.\n",
    "SameDayCutoffTime\tTIME\t|\tTime of day for same-day delivery cutoff.\n",
    "TimeZone\t|    STRING\t|\tTime zone of the facility\n",
    "TimeDifferenceHours\t|    INT64\t|\tNumeric time difference in hours from a reference time zone\n",
    "FacilitySortCode\t|    STRING\t|\tCode used to sort or categorize the facility.\n",
    "IsLocalServiceCenter\t|    BOOL\t|\tflag indicating if the facility is a local service center.\n",
    "IsGlobalServiceCenter\t|    BOOL\t|\tflag indicating if the facility is a global service center.\n",
    "IsSortCenter\t|    BOOL\t|\tflag indicating if the facility is a sort center.\n",
    "IsAdminServiceCenter\t|    BOOL\t|\tflag indicating if the facility is an administrative service center.\n",
    "IsCustomerServiceCenter\t|    BOOL\t|\tflag indicating if the facility is a customer service center.\n",
    "IsSortCodeOnly\t|    BOOL\t|\tflag indicating if the facility is identified by sort code only.\n",
    "IsContractorRunSatellite\t|    BOOL\t|\tflag indicating if the facility is a contractor-run satellite location.\n",
    "IsExpressMessenger\t|    BOOL\t|\tflag indicating if the facility handles express messenger services.\n",
    "IsDDUOnly\t|    BOOL\t|\tflag indicating if the facility is a Destination Delivery Unit only.\n",
    "IsExpressMessengerInternational\t|    BOOL\t|\tflag indicating if the facility handles international express messenger services.\n",
    "WindowsTimeZone\t|    STRING\t|\tTime zone used in Windows systems for the facility.\n",
    "ParentFacilityKey\t|    INT64\t|\tNumeric identifier for the parent facility, if applicable.\n",
    "BusinessUnit\t|    STRING\t|\tBusiness unit or segment associated with the facility.\n",
    "\n",
    "\n",
    "dim_date\n",
    "This table provides a date dimension with various attributes for calendar and fiscal periods. This table is essential for time-based analysis and reporting, offering indicators for holidays  and peak periods.\n",
    "column_name | data_type | description\n",
    "DateKey\t|    INT64\t|\tA unique identifier for each date\n",
    "Calendardate\t|    DATE\t|\tThe specific calenar date\n",
    "CalendarDatetime    |    DATETIME\t|\tThe timestamp of the date\n",
    "CalendarDayOfYear\t|    INT64\t|\tThe day of the year in the calendar\n",
    "CalendarMonthName\t|    STRING\t|\tThe name of the calendar month\n",
    "CalendarMonthNumber\t|    INT64\t|\tThe month the number in the calendar\n",
    "CalendarQuarter\t|    INT64\t|\tThe quarter of the calendar year\n",
    "CalendarDayOfWeekName\t|    STRING\t|\tThe name of the day of the week.\n",
    "CalendarDayOfWeek\t|    INT64\t|\tThe day of the week in the calendar\n",
    "CalendarDayOfMonth\t|    INT64\t|\tThe day of the month in the calendar\n",
    "CalendarWeekOfMonth\t|    INT64\t|\tThe week of the month in the calendar\n",
    "CalendarWeekOfYear\t|    INT64\t|\tThe week of the year in the calendar\n",
    "CalendarYear\t|    INT64\t|\tThe numeric year in the calendar\n",
    "CalendarYearMonthAsInteger\t|    INT64\t|\tA concatenation of the year and month as an integer.\n",
    "CalendarYearQuarterAsInteger\t|    INT64\t|\tA concatenation of the year and quarter as an integer.\n",
    "FirstDayOfCalendarMonthIndicator\t|    STRING\t|\tIndicator if the date is the first day of the calendar month.\n",
    "FirstdateOfCalendarMonth\t|    DATE\t|\tThe first date of the calendar month.\n",
    "LastDayOfCalendarMonthIndicator\t|    STRING\t|\tIndicator if the date is the last day of the calendar month.\n",
    "LastdateOfCalendarMonth\t|    DATE\t|\tThe last date of the calendar month.\n",
    "FirstdateOfCalendarWeek\t|    DATE\t|\tThe first date of the calendar week.\n",
    "LastdateOfCalendarWeek\t|    DATE\t|\tThe last date of the calendar week.\n",
    "HolidayIndicator\t|    STRING\t|\tIndicates if the date is a holiday\n",
    "HolidayName\t|    STRING\t|\tThe name of the holiday\n",
    "HolidayIndicatorObserved\t|    STRING\t|\tIndicator if the date is an observed holiday.\n",
    "HolidayNameObserved\t|    STRING\t|\tThe name of the observed holiday.\n",
    "WeekdayWeekend\t|    STRING\t|\tIndicates if the day is Weekday or Weekend\n",
    "VolumeTierEnddateIndicator\t|    STRING\t|\tIndicator if the date is the end of a volume tier.\n",
    "PeakIndicator\t|    STRING\t|\tIndicator if the date is during a peak period.\n",
    "Fiscaldate\t|    DATE\t|\tThe specific fiscal date\n",
    "FiscalDayOfYear\t|    INT64\t|\tThe day number within the fiscal year\n",
    "FiscalMonthName\t|    STRING\t|\tThe name of the fiscal month\n",
    "FiscalMonthNumber\t|    INT64\t|\tThe numeric representation of the fiscal month\n",
    "FiscalQuarter\t|    INT64\t|\tThe quarter of the fiscal year\n",
    "FiscalDayOfMonth\t|    INT64\t|\tThe numeric day within the fiscal month.\n",
    "FiscalYear\t|    INT64\t|\tThe fiscal year\n",
    "FiscalYearMonthAsInteger\t|    INT64\t|\tA concatenation of the fiscal year and month as an integer.\n",
    "FiscalYearQuarterAsInteger\t|    INT64\t|\tA concatenation of the fiscal year and quarter as an integer.\n",
    "FirstDayOfFiscalMonthIndicator\t|    STRING\t|\tIndicator if the date is the first day of the fiscal month.\n",
    "FirstdateOfFiscalMonth\t|    DATE\t|\tThe first date of the fiscal month.\n",
    "LastDayOfFiscalMonthIndicator\t|    STRING\t|\tIndicator if the date is the last day of the fiscal month.\n",
    "LastdateOfFiscalMonth\t|    DATE\t|\tThe last date of the fiscal month.\n",
    "FirstdateOfReportWeek\t|    DATE\t|\tThe first date of the report week.\n",
    "LastdateOfReportWeek\t|    DATE\t|\tThe last date of the report week.\n",
    "ReportDayOfWeekName\t|    STRING\t|\tThe name of the report day of the week.\n",
    "ReportDayOfWeek\t|    INT64\t|\tThe numeric representation of the report day of the week.\n",
    "ReportWeekOfYear\t|    INT64\t|\tThe week number within the report year.\n",
    "ReportWeekLabel\t|    STRING\t|\tA label for the report week.\n",
    "ReportYear\t|    INT64\t|\tThe report year\n",
    "ReportYearWeekAsInteger\t|    INT64\t|\tA concatenation of the report year and week as an integer.\n",
    "AlternativeReportWeekOfYear\t|    INT64\t|\tAn alternative week number within the report year.\n",
    "AlternativeReportWeekLabel\t|    STRING\t|\tAn alternative label for the report week.\n",
    "AlternativeReportYear\t|    INT64\t|\tAn alternative report year\n",
    "AlternativeReportYearWeekAsInteger\t|    INT64\t|\tA concatenation of the alternative report year and week as an integer.\n",
    "\n",
    "\n",
    "dim_customer\n",
    "This table holds comprehensive customer information, including customer names, industry types , and delivery preferences. The table also tracks billing schedules, signature requirements, and operational parameters.\n",
    "column_name | data_type | description\n",
    "CustomerName\t|    STRING\t|\tThe name of the customer\n",
    "Industry\t|    STRING\t|\tThe name of industry the customer belongs to\n",
    "ActiveFlag\t|    BOOL\t|\tIndicates if the customer is active.\n",
    "AttemptBillability\t|    INT64\t|\tThe count of billable attempts for the customer.\n",
    "AuditScanReport\t|    BOOL\t|\tIndicates if audit scan reporting is enabled for the customer.\n",
    "AuthorizedCheatingScans\t|    BOOL\t|\tIndicates if cheating scans are authorized for the customer.\n",
    "AuthorizedDeliveryMonday\t|    BOOL\t|\tIndicates if delivery is authorized on Mondays.\n",
    "AuthorizedDeliveryTuesday\t|    BOOL\t|\tIndicates if delivery is authorized on Tuesdays.\n",
    "AuthorizedDeliveryWednesday\t|    BOOL\t|\tIndicates if delivery is authorized on Wednesdays.\n",
    "AuthorizedDeliveryThursday\t|    BOOL\t|\tIndicates if delivery is authorized on Thursdays.\n",
    "AuthorizedDeliveryFriday\t|    BOOL\t|\tIndicates if delivery is authorized on Fridays.\n",
    "AuthorizedDeliverySaturday\t|    BOOL\t|\tIndicates if delivery is authorized on Saturdays.\n",
    "AuthorizedDeliverySunday\t|    BOOL\t|\tIndicates if delivery is authorized on Sundays.\n",
    "AuthorizedServices\t|    STRING\t|\tThe list of services authorized for the customer.\n",
    "BillingSchedule\t|    STRING\t|\tThe billing schedule for the customer.\n",
    "CalculatedDeliveryByMethod\t|    STRING\t|\tThe method used to calculate delivery time.\n",
    "DefaultSignatureType\t|    STRING\t|\tThe default type of signature required\n",
    "DefaultWeight\t|    INT64\t|\tThe default weight used for shipments\n",
    "DimensionFactor\t|    INT64\t|\tThe dimension factor used for shipping\n",
    "DimensionMinimumVolume\t|    INT64\t|\tThe minimum volume considered for dimensional pricing.\n",
    "GuesstimateTransitTime\t|    STRING\t|\tThe estimated transit time for shipments.\n",
    "InjectionPostalCode\t|    STRING\t|\tThe postal code where goods are injected into the system.\n",
    "ManifestForceSignatureRequired\t|    BOOL\t|\tIndicates if a signature is required on the manifest.\n",
    "ManifestReturnsAuthorized\t|    BOOL\t|\tIndicates if returns are authorized on the manifest.\n",
    "MaxAttempts\t|    INT64\t|\tThe maximum number of delivery attempts allowed\n",
    "MaxTransitDayForForward\t|    INT64\t|\tThe maximum number of transit days for forward delivery\n",
    "NextDayDeliveryCutOffTimeOther\t|    INT64\t|\tThe cut-off time for next-day delivery for other shipments.\n",
    "NextDayDeliveryCutOffTimeRegularDelivery\t|    INT64\t|\tThe cut-off time for next-day regular delivery.\n",
    "NextDayDeliveryCutOffTimeSamedayDelivery\t|    INT64\t|\tThe cut-off time for same-day delivery.\n",
    "ReuseBarcode\t|    BOOL\t|\tIndicated if the barcodes can be reused\n",
    "SignatureWaiveable\t|    BOOL\t|\tIndicates if the signature requirements can be waived\n",
    "UseCustomerDimensionForPricing\t|    BOOL\t|\tIndicates if the customer specific dimensions are used for pricing\n",
    "AccountDirector\t|    STRING\t|\tThe name of the account director managing the customer\n",
    "AccountOwner\t|    STRING\t|\tThe name of the account owner managing the customer\n",
    "AMTGroup\t|    STRING\t|\tThe account management team group associated with the customer.\n",
    "CustomerStart\t|    STRING\t|\tThe start daye of the customer relationship with the company\n",
    "FinanceTag\t|    STRING\t|\tA finance tag associated with the customer\n",
    "DefaultCustomerRollupCode\t|    STRING\t|\tThe default rollup customer code in reporting or billing systems.\n",
    "\n",
    "\n",
    "dim_service\n",
    "This table stores information about the various services offered, including service identifiers and descriptions. The table also includes legacy data and flags to determine how services are counted and billed.\n",
    "column_name | data_type | description\n",
    "ServiceId\t|    STRING\t|\tA unique identifier for the service.\n",
    "ServiceName\t|    STRING\t|\tThe name of the service.\n",
    "LegacyName\t|    STRING\t|\tThe name of the Legacy company of the service\n",
    "PickupOrDeliveryRevenue\t|    STRING\t|\tRevenue associated with pickup or delivery for the service.\n",
    "IsCountedAsPiece\t|    BOOL\t|\tIndicates if the service is counted as a separate piece.\n",
    "TransconNameId\t|    STRING\t|\tIdentifier for the name of the transcontinental service.\n",
    "TransconTypeId\t|    INT64\t|\tIdentifier for the type of transcontinental service.\n",
    "ServiceDefinition\t|    STRING\t|\tA description or definition of the service.\n",
    "LegacyCompany\t|    STRING\t|\tThe legacy company associated with the service.\n",
    "\n",
    "\n",
    "dim_account\n",
    "This table contains information about customer accounts, including account identifiers, account names, and statuses. It also tracks the relationship of accounts with customers and services, as well as categorization.\n",
    "column_name | data_type | description\n",
    "AccountId\t|    STRING\t|\tA unique identifier for the account.\n",
    "AardvarkId\t|    STRING\t|\tInternal ID used to distinguish certain accounts\n",
    "CustomerParentId\t|    STRING\t|\tThe identifier of the parent customer.\n",
    "FacilityCodeId\t|    STRING\t|\tThe identifier for the facility code associated with the account.\n",
    "ServiceCodeId\t|    STRING\t|\tThe identifier for the service code linked to the account.\n",
    "AccountName\t|    STRING\t|\tThe name of the account.\n",
    "AccountStatus\t|    INT64\t|\tThe status of the account, likely represented numerically.\n",
    "ParentAccount\t|    STRING\t|\tThe identifier of the parent account, if applicable.\n",
    "SubCustomerCode\t|    STRING\t|\tA code representing a sub-customer or sub-division of the account.\n",
    "RollupCode\t|    STRING\t|\tA code used for aggregating or \"rolling up\" accounts in reporting.\n",
    "RollupName\t|    STRING\t|\tThe name associated with the rollup code.\n",
    "LineOfBusCategory\t|    STRING\t|\tThe business category the account falls under.\n",
    "LineOfBusSubCategory\t|    STRING\t|\tThe subcategory within the business line.\n",
    "LegacyCompany\t|    STRING\t|\tThe legacy company (either LaserShip or Ontrac) associated with the account.\n",
    "\n",
    "\n",
    "Think step by step, then generate an consolidated SQL query that answers the user's query without any errors.\n",
    "\n",
    "Here are some examples for reference:\n",
    "\n",
    "Question:\n",
    "What is the top performing customer this week?\n",
    "\n",
    "SQL:\n",
    "\n",
    "SELECT COUNT(*) AS NumVolume, CustomerName\n",
    "FROM onelook_test.fact_package fp\n",
    "JOIN onelook_test.dim_customer dc ON fp.CustomerKey = dc.CustomerKey\n",
    "JOIN onelook_test.dim_date dd ON fp.FactDateKey = dd.DateKey\n",
    "WHERE dd.CalendarDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\n",
    "GROUP BY CustomerName\n",
    "ORDER BY NumVolume DESC\n",
    "LIMIT 1;\n",
    "\n",
    "\n",
    "Question:\n",
    "What are the least utilized zips for Abercrombie (CXO4)?\n",
    "\n",
    "SQL:\n",
    "\n",
    "SELECT COUNT(*) AS NumVolume, DestinationZipCode\n",
    "FROM onelook_test.fact_package fp\n",
    "JOIN onelook_test.dim_customer dc ON fp.CustomerKey = dc.CustomerKey\n",
    "JOIN onelook_test.dim_date dd ON fp.FactDateKey = dd.DateKey\n",
    "WHERE dd.CalendarDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 40 DAY)\n",
    "    AND dc.CustomerId = 'CXO4'\n",
    "GROUP BY DestinationZipCode\n",
    "ORDER BY NumVolume ASC\n",
    "LIMIT 10;\n",
    "\n",
    "Question:\n",
    "what is the  top  and  Bottom  Volume performing customer   this week , month, year\n",
    "\n",
    "SQL:\n",
    "\n",
    "WITH WeeklyVolume AS (\n",
    "  SELECT\n",
    "    dc.CustomerName,\n",
    "    COUNT(*) AS WeeklyVolume\n",
    "  FROM onelook_test.fact_package fp\n",
    "  JOIN onelook_test.dim_customer dc\n",
    "    ON fp.CustomerKey = dc.CustomerKey\n",
    "  JOIN onelook_test.dim_date dd\n",
    "    ON fp.FactDateKey = dd.DateKey\n",
    "  WHERE\n",
    "    dd.CalendarDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\n",
    "  GROUP BY\n",
    "    dc.CustomerName\n",
    "), MonthlyVolume AS (\n",
    "  SELECT\n",
    "    dc.CustomerName,\n",
    "    COUNT(*) AS MonthlyVolume\n",
    "  FROM onelook_test.fact_package fp\n",
    "  JOIN onelook_test.dim_customer dc\n",
    "    ON fp.CustomerKey = dc.CustomerKey\n",
    "  JOIN onelook_test.dim_date dd\n",
    "    ON fp.FactDateKey = dd.DateKey\n",
    "  WHERE\n",
    "    dd.CalendarDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\n",
    "  GROUP BY\n",
    "    dc.CustomerName\n",
    "), YearlyVolume AS (\n",
    "  SELECT\n",
    "    dc.CustomerName,\n",
    "    COUNT(*) AS YearlyVolume\n",
    "  FROM onelook_test.fact_package fp\n",
    "  JOIN onelook_test.dim_customer dc\n",
    "    ON fp.CustomerKey = dc.CustomerKey\n",
    "  JOIN onelook_test.dim_date dd\n",
    "    ON fp.FactDateKey = dd.DateKey\n",
    "  WHERE\n",
    "    dd.CalendarDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 365 DAY)\n",
    "  GROUP BY\n",
    "    dc.CustomerName\n",
    ")\n",
    "SELECT\n",
    "  w.CustomerName,  -- Use alias w for WeeklyVolume\n",
    "  w.WeeklyVolume,  -- Correctly reference WeeklyVolume\n",
    "  m.MonthlyVolume, -- Correctly reference MonthlyVolume\n",
    "  y.YearlyVolume   -- Correctly reference YearlyVolume\n",
    "FROM WeeklyVolume w  -- Alias WeeklyVolume as w\n",
    "JOIN MonthlyVolume m ON w.CustomerName = m.CustomerName  -- Alias MonthlyVolume as m\n",
    "JOIN YearlyVolume y ON w.CustomerName = y.CustomerName  -- Alias YearlyVolume as y\n",
    "ORDER BY\n",
    "  w.WeeklyVolume DESC\n",
    "LIMIT 1;\n",
    "\n",
    "{embeddings_DataDictionary.columns}\n",
    "\n",
    "Context:\n",
    " - Text Context:\n",
    " {final_context_text}\n",
    "\n",
    "\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate the SQL query\n",
    "response = get_gemini_response(\n",
    "    multimodal_model,\n",
    "    model_input=[prompt],\n",
    "    stream=True,\n",
    "    generation_config=GenerationConfig(temperature=0, top_p = 0, max_output_tokens=2048),\n",
    ")\n",
    "\n",
    "# Ensure the response is trimmed of any unexpected whitespace\n",
    "sql_query = response.strip()\n",
    "\n",
    "# Extract the SQL query using regular expression\n",
    "match = re.search(r\"```sql\\n(.*)\\n```\", sql_query, flags=re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    extracted_sql = match.group(1)\n",
    "    print(extracted_sql)\n",
    "else:\n",
    "    print(\"No SQL query found in the response.\")\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(project='prj-ot-dev-bqsandbox-000001')\n",
    "\n",
    "print('\\n')\n",
    "# Execute the SQL query\n",
    "try:\n",
    "    response_query_job = client.query(extracted_sql)\n",
    "    df = response_query_job.to_dataframe()\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027ea56-f9b6-4ccc-bb5e-a422c15d0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import re\n",
    "\n",
    "# Read the parquet file\n",
    "embeddings_DataDictionary = pd.read_parquet(\"embeddings_DataDictionary.pq\")\n",
    "\n",
    "# Initialize history and context storage\n",
    "conversation_history = []\n",
    "\n",
    "def update_conversation_history(question, sql_query=None, results=None):\n",
    "    conversation_history.append({\"question\": question, \"sql_query\": sql_query, \"results\": results})\n",
    "\n",
    "# Query for the question\n",
    "query = \"\"\"\\\n",
    "Question:\n",
    " -  How many packages were delivered without a signature last month?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Get matching results chunks data\n",
    "\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    embeddings_DataDictionary,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=10,\n",
    "    chunk_text=False,\n",
    ")\n",
    "\n",
    "context_text = []\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.append(value[\"column_name\"] + \" \" + value[\"data_type\"] + \" \" + value[\"description\"] + \" \" + value[\"table_name\"])\n",
    "final_context_text = \"\\n\".join(context_text)\n",
    "\n",
    "def get_prompt_for_gemini(query, context_text):\n",
    "    return f\"\"\"\\\n",
    "Instructions: You are an AI assistant designed to generate SQL queries for the Onelook BigQuery database.\n",
    "Write a SQL Query. Write it in such a way that this SQL can be further executed in BigQuery.\n",
    "If no limit or offset is mentioned and if the data is expected to be large, use 0 for the offset and 10 for the limit as default. User will give in the question if they need a specific page.\n",
    "Use the schema namespace `prj-ot-dev-bqsandbox-000001.onelook_test` as default for all tables when writing the SQL Query. Use joins if needed.\n",
    "\n",
    "You have access to the following tables:\n",
    "dim_eventtype\n",
    "The table lists and categorizes event types used to track and manage shipment statuses and processes within the logistics network. The table includes unique event identifiers, labels, and status indicators.It also links events to their respective systems and processes.\n",
    "column_name | data_type | description\n",
    "EventTypeKey\t|    INT64\t|\tUnique identifier for the type of event\n",
    "SourceSystemKey\t|    INT64\t|\tUnique identifier for the source system of the event\n",
    "EventType\t|    STRING\t|\tDescribes the type of event\n",
    "EventModifier\t|    STRING\t|\tIn combination with EventType, describes the type of the event\n",
    "LegacyEventTypeId\t|    STRING\t|\tIdentifier used in a previous system for the event type.\n",
    "LegacyEventModifierId\t|    STRING\t|\tIdentifier used in a previous system for the event modifier.\n",
    "EventCategory\t|    STRING\t|\tCategory classification of the event.\n",
    "Label\t|    STRING\t|\tLabel or name assigned to the event type.\n",
    "ShortText\t|    STRING\t|\tBrief description of the event type.\n",
    "LongText\t|    STRING\t|\tDetailed description of the event type\n",
    "PublicLabel\t|    STRING\t|\tLabel intended for public display.\n",
    "PublicShortText\t|    STRING\t|\tBrief description intended for public display.\n",
    "PublicLongText\t|    STRING\t|\tDetailed description intended for public display.\n",
    "Controllable\t|    STRING\t|\tIndicator of whether the event type is controllable\n",
    "IsCompletionEvent\t|    BOOL\t|\tflag indicating if the event is a completion event.\n",
    "IncludeInFactPackageProcess\t|    BOOL\t|    Boolean flag indicating if the event is included in the fact package process.\n",
    "IsFacilityTouch\t|    BOOL\t|    Boolean flag indicating if the event involves a facility touch.\n",
    "\n",
    "\n",
    "dim_customerfacility\n",
    "The table captures details about customer-specific facilities, including facility identifiers and geographic details. It also categorizes the type of location and links facilities to their respective customers.\n",
    "column_name | data_type | description\n",
    "CustomerFacilityKey\t|    INT64\t|\tUnique identifier for the customer facility.\n",
    "CustomerId\t|    STRING\t|\tunique identifier for the customer.\n",
    "CustomerKey\t|    INT64\t|\tThe customer key associated with this facilit\n",
    "SourceSystemKey\t|    INT64\t|\tIdentifier for the source system of the data.\n",
    "CustomerFacilityId\t|    STRING\t|\tIdentifier for the customer facility in the source system.\n",
    "LocationType\t|    STRING\t|\tDescription of the type or category of the location.\n",
    "Address1\t|    STRING\t|\tPrimary address line for the customer facility.\n",
    "Address2\t|    STRING\t|\tSecondary address line for the customer facility.\n",
    "City\t|    STRING\t|\tCity where the customer facility is located\n",
    "State\t|    STRING\t|\tState or region where the customer facility is located.\n",
    "ZipCode\t|    STRING\t|\tPostal code for the customer facility address.\n",
    "Country\t|    STRING\t|\tCountry where the customer facility is located.\n",
    "InjectionFacilities\t|    STRING\t|\tDescription or identifier related to injection facilities associated with the customer.\n",
    "InjectionPostalCode\t|    STRING\t|\tPostal code for the injection facilities.\n",
    "\n",
    "\n",
    "fact_package\n",
    "This table stores detailed records of shipments and package tracking events. It includes key details such as package identifiers, dates , and status flags. The table also captures location information , transit times, and financial data.\n",
    "column_name | data_type | description\n",
    "FactPackageKey\t|    INT64\t|\tUnique ID for this record\n",
    "FactId\t|    STRING\t|\tUnique ID for this record\n",
    "FactDateKey\t|    INT64\t|\tdate key that this record was inserted into FactPackage\n",
    "SourceSystemKey\t|    INT64\t|\tunique identifier assigned to Source System\n",
    "AccountKey\t|    INT64\t|\tAccountKey associated with this package\n",
    "CustomerKey\t|    INT64\t|\tThe customer who shipped this package\n",
    "ServiceKey\t|    INT64\t|\tThe type of service the package was delivered with\n",
    "DestinationBranchKey\t|    INT64\t|\tunique identifier assigned to the Destination Branch\n",
    "CustomerFacilityKey\t|    INT64\t|\tunique identifier assigned to the Customer Facility\n",
    "RunId  |\tFLOAT64\t|\tunique identifier for the specific run\n",
    "BarcodeId\t|    STRING\t|\tIdentification number generated on the package\n",
    "OrderNumber\t|    STRING\t|\tunique identifier assigned to each transaction made\n",
    "InvoiceNumber\t|    STRING\t|\tunique identifier assigned to each invoice generated by a business.Its used for tracking, reference, and record-keeping purposes\n",
    "ManifestDateKey\t|   INT64\t|\tunique identifier assigned to the date and time when a shipping manifest was created for a package\n",
    "ManifestDatetime\t|   The date and time when a shipping manifest was created for a package\n",
    "OriginSystem\t|    STRING\t|\tOrigin or warehouse of the package\n",
    "FirstSortFacilityKey\t|    INT64\t|\tUnique identifier for theFirst sorting facility the package passed through.\n",
    "FirstSortDateKey\t|    INT64\t|\tunique identifier assigned to the earliest date if the sorting is done based on a date field\n",
    "FirstSortDateTime\t|   DATETIME\t|\tTimestamp of First Sorting\n",
    "FirstIntermediateSortFacilityKey\t|    INT64\t|\tIdentifies the facility of First Intermediate sort\n",
    "FirstIntermediateSortDateKey\t|    INT64\t|\tIdentifies the date of First Intermediate Sort\n",
    "FirstIntermediateSortDatetime  |    DATETIME\t|\tTimestamp of First Intermediate Sort\n",
    "SecondIntermediateSortFacilityKey\t|    INT64\t|\tIdentifies the facility of Second Intermediate sort\n",
    "SecondIntermediateSortDateKey\t|    INT64\t|\tIdentifies the date of Second Intermediate Sort\n",
    "SecondIntermediateSortDatetime    |    DATETIME\t|\tTimestamp of Second Intermediate Sort\n",
    "LastSortFacilityKey\t|    INT64\t|\tunique identifier assigned to the Facility of last sorting\n",
    "LastSortDateKey\t|    INT64\t|\tunique identifier assigned to date of last sorting\n",
    "LastSortDatetime   |    DATETIME\t|\tTimestamp of Last Sorting\n",
    "DestinationFacilityKey\t|    INT64\t|\tunique identifier assigned to indicate destination Facility\n",
    "DestinationDateKey\t|    INT64\t|\tunique identifier assigned to the date  when a package or shipment is expected to arrive at its final destination\n",
    "DestinationDatetime    |    DATETIME\t|\tTimestamp of the package expected to arrive at its final destination\n",
    "SortFacilityCount\t|    INT64\t|\tNumber of sorting facilities the package passed through.\n",
    "LoadDateKey\t|    INT64\t|\tunique identifier assigned to the date  when a shipment is loaded\n",
    "LoadDatetime\t|    DATETIME\t|\tthe date and time when a shipment is loaded\n",
    "DepartedDateKey\t|    INT64\t|\tunique identifier assigned to the date when a shipment departs from\n",
    "DepartedDatetime  |    DATETIME\t|\tthe date and time when a shipment departs from a facility or location.\n",
    "FirstAttemptFacilityKey\t|    INT64\t|\tunique identifier assigned to the facility where the first delivery attempt was made.\n",
    "FirstAttemptDateKey\t|    INT64\t|\tunique identifier assigned to  the date of the first delivery attempt.\n",
    "FirstAttemptDatetime\t|    DATETIME\t|\tthe date and time of the first delivery attempt.\n",
    "DeliveryFacilityKey\t|    INT64\t|\tunique identifier assigned to for the facility where the delivery was made.\n",
    "DeliveryDateKey\t|    INT64\t|\tunique identifier assigned to the date when a delivery was made.\n",
    "DeliveryDatetime |    DATETIME\t|\tthe date and time when a delivery was made.\n",
    "CustomerCommitmentDateKey\t|    INT64\t|  DateKey for when we have committed to delivering the package\n",
    "isNetworkOntime\t|    BOOL\t|\tIndicates if the package was on time according to network standards.\n",
    "isDestinationFacilityOntime\t|    BOOL\t|\tIndicates if the package will reach its destination facility on time\n",
    "isCustomerOntime\t|    BOOL\t|\tIndicates if the package was on time to meet the CustomerCommitment\n",
    "DaysLate\t|    INT64\t|\tThe number of days a delivery was late\n",
    "DaysEarly\t|    INT64\t|\tthe number of days a delivery was early\n",
    "ActualTransitDays\t|    INT64\t|\tRepresent the actual number of days a shipment was in transit (excludes holidays)\n",
    "ActualTransitDaysManifest\t|    INT64\t|\trepresent the actual number of days a shipment was in transit, as recorded in the manifest.\n",
    "TransitCategory\t|    STRING\t|\tTransit Category for the Package\n",
    "TotalDeliveryCost\tFLOAT64\t|\tThe total delivery cost for this package\n",
    "RevenueExclFuel\tFLOAT64\t|\tThe revenue generated from the delivery, excluding the cost of fuel.\n",
    "Fuel\tFLOAT64\t|\tThe cost of fuel used for the delivery.\n",
    "VolumeCount\t|    BOOL\t|\tThe count of items for this delivery\n",
    "BillableTransactionDateKey\t|    INT64\t|\tdate of the billable transaction.\n",
    "DestinationZipCode\t|    STRING\t|\tThe zip code of the delivery destination.\n",
    "OriginZipCode\t|    STRING\t|\tThe zip code where the shipment originated.\n",
    "PriceZone\t|    INT64\t|\tA classification based on pricing, likely related to shipping zones.\n",
    "hasVPOD\t|    BOOL\t|\tIndicates if a voice proof of delivery was obtained\n",
    "hasSignature\t|    BOOL\t|\tIndicates if the shipment has a proof of delivery.\n",
    "isSignatureRequired\t|    BOOL\t|\tIndicates if the shipment required a Signature\n",
    "hasManualDelivery\t|    BOOL\t|\tIndicates if the delivery was manual.\n",
    "hasScannedDelivery\t|    BOOL\t|\tIndicates if the delivery was scanned\n",
    "isShortage\t|    BOOL\t|\tIndicates if the delivery was shortage.\n",
    "isDeleted\t|    BOOL\t|\tIndicates if the package was deleted\n",
    "WeatherFlag\t|    BOOL\t|\tIndicates if the package was affected by weather.\n",
    "WeatherFlagFacilityKey\t|    INT64\t|\tIdentifies the Facility affected by weather.\n",
    "WeatherFlagDateKey\t|    INT64\t|\tIdentifies the date of Weather impact\n",
    "WeatherFlagDatetime   |    DATETIME\t|\tTimestamp of the weather impact\n",
    "LostFlag\t|    BOOL\t|\tIndicates if the shipment was lost\n",
    "LostFlagFacilityKey\t|    INT64\t|\tIdentifies the facility where the package was lost\n",
    "LostFlagDateKey\t|    INT64\t|  DATE of the package loss\n",
    "LostFlagDatetime    |    DATETIME\t|\tTimestamp of the package loss\n",
    "DamageFlag\t|    BOOL\t|\t Indicates if the package was damaged.\n",
    "DamageFlagFacilityKey\t|    INT64\t|\tIdentifies the Facility where the package was damaged.\n",
    "DamageFlagDateKey\t|    INT64\t|\tIdentifies the date of package damage.\n",
    "DamageFlagDatetime   |    DATETIME\t|\tTimestamp of the package damage\n",
    "MissortFlag\t|    BOOL\t|\tIndicates if the package was missorted.\n",
    "MissortFlagFacilityKey\t|    INT64\t|\tIdentifies the Facility where the package was missorted.\n",
    "MissortFlagDateKey\t|    INT64\t|\tIdentifies the date when the package was missorted\n",
    "MissortFlagDatetime    |    DATETIME\t|\tTimestamp of Package missort\n",
    "OnHoldFlag\t|    BOOL\t|\tindicates if the package was on missort\n",
    "OnHoldFlagFacilityKey\t|    INT64\t|\tIdentifies where the package was on hold\n",
    "OnHoldFlagDateKey\t|    INT64\t|\tIdentifies the date of package which was on hold\n",
    "OnHoldFlagDatetime    |    DATETIME\t|\tTimestamp of the package on hold\n",
    "RTSFlag\t|    BOOL\t|\tIndicated if the package was returned to sender\n",
    "RTSFlagFacilityKey\t|    INT64\t|\tIdentifies if the package was returned to sender\n",
    "RTSFlag|    DATEKey\t|    INT64\t|\tIdentifies the date of package return to sender\n",
    "RTSFlagDatetime    |    DATETIME\t|\tTimestamp of the package return\n",
    "LastExceptionOrAttempt|    DATEKey\t|    INT64\t|\tIdentifies the date of last exception or attempt occurred\n",
    "LastExceptionOrAttemptDatetime\t    |    DATETIME\t|\tTimestamp of the date of last exception or attempt occurred\n",
    "LastExceptionOrAttemptFacilityKey\t|    INT64\t|\tIdentifies the facility of last exception or attempt occurred\n",
    "LastExceptionOrAttemptEventTypeKey\t|    INT64\t|\tIdentifies the Type of the last exception or attempt.\n",
    "CurrentPackageStatusEventTypeKey\t|    INT64\t|\tIdentifies the Current status of the package.\n",
    "WestInsertedLogId\t|    INT64\t|\tIdentifier for a log entry made in the western region.\n",
    "EastInsertedLogId\t|    INT64\t|\tIdentifier for a log entry made in the eastern region.\n",
    "WestInsertedDatetime    |    DATETIME\t|\tTimestamp for when a log entry was made in the western region.\n",
    "EastInsertedDatetime    |    DATETIME\t|\tTimestamp for when a log entry was made in the eastern region.\n",
    "RunCreatedLocalDateKey\t|    INT64\t|\tThe local date when a batch of operations or events was created.\n",
    "RunCreatedLocalDatetime    |    DATETIME\t|\tThe Local timestamp when a batch of operations or events was created.\n",
    "DestinationLocationType\t|    STRING\t|\tType of the destination location of a package\n",
    "\n",
    "\n",
    "dim_branch\n",
    "This table represents branch details, including unique identifiers and names. It is linked to facilities and other location-based operations, providing a geographic and operational structure for the transportation network.\n",
    "column_name | data_type | description\n",
    "BranchKey\t|    INT64\t|\tNumeric identifier for the branch.\n",
    "SourceSystemKey\t|    INT64\t|\tNumeric key representing the source system of the branch data.\n",
    "BranchCodeId\t|    STRING\t|\tIdentifier for the branch code\n",
    "BranchName\t|    STRING\t|\tName of the branch\n",
    "\n",
    "\n",
    "dim_facility\n",
    "This table contains detailed facility information, including identifiers, geographic location , and operational details. It also tracks facility relationships and time-related information.\n",
    "column_name | data_type | description\n",
    "FacilityKey\t|    INT64\t|\tNumeric identifier for the facility.\n",
    "BranchKey\t|    INT64\t|\tNumeric key representing the branch to which the facility belongs.\n",
    "SourceSystemKey\t|    INT64\t|\tNumeric key for the source system of the facility data.\n",
    "FacilityId\t|    STRING\t|\tAlphanumeric identifier for the facility.\n",
    "WestFacility\t|    STRING\t|\tIdentifier or code for a facility in the western region.\n",
    "EastFacility\t|    STRING\t|\tIdentifier or code for a facility in the eastern region.\n",
    "BranchCode\t|    STRING\t|\tCode associated with the branch of the facility.\n",
    "LegacyCompany\t|    STRING\t|\tThe legacy company associated with the facility\n",
    "FacilityName\t|    STRING\t|\tName of the facility\n",
    "IsActive\t|    INT64\t|\tNumeric flag indicating if the facility is active (e.g., 1 for active, 0 for inactive).\n",
    "Region\t|    STRING\t|\tGeographic or operational region of the facility.\n",
    "Division\t|    STRING\t|\tDivision or sector to which the facility belongs.\n",
    "Address1\t|    STRING\t|\tPrimary address line for the facility.\n",
    "Address2\t|    STRING\t|\tSecondary address line for the facility.\n",
    "City\t|    STRING\t|\tCity where the facility is located.\n",
    "State\t|    STRING\t|\tState where the facility is located.\n",
    "ZipCode\t|    STRING\t|\tPostal code for the facility address\n",
    "Latitude\tFLOAT64\t|\tGeographic latitude coordinate of the facility\n",
    "Longitude\tFLOAT64\t|\tGeographic longitude coordinate of the facility\n",
    "RegularDeliveryCutoffTime\tTIME\t|\tTime of day for regular delivery cutoff.\n",
    "SameDayCutoffTime\tTIME\t|\tTime of day for same-day delivery cutoff.\n",
    "TimeZone\t|    STRING\t|\tTime zone of the facility\n",
    "TimeDifferenceHours\t|    INT64\t|\tNumeric time difference in hours from a reference time zone\n",
    "FacilitySortCode\t|    STRING\t|\tCode used to sort or categorize the facility.\n",
    "IsLocalServiceCenter\t|    BOOL\t|\tflag indicating if the facility is a local service center.\n",
    "IsGlobalServiceCenter\t|    BOOL\t|\tflag indicating if the facility is a global service center.\n",
    "IsSortCenter\t|    BOOL\t|\tflag indicating if the facility is a sort center.\n",
    "IsAdminServiceCenter\t|    BOOL\t|\tflag indicating if the facility is an administrative service center.\n",
    "IsCustomerServiceCenter\t|    BOOL\t|\tflag indicating if the facility is a customer service center.\n",
    "IsSortCodeOnly\t|    BOOL\t|\tflag indicating if the facility is identified by sort code only.\n",
    "IsContractorRunSatellite\t|    BOOL\t|\tflag indicating if the facility is a contractor-run satellite location.\n",
    "IsExpressMessenger\t|    BOOL\t|\tflag indicating if the facility handles express messenger services.\n",
    "IsDDUOnly\t|    BOOL\t|\tflag indicating if the facility is a Destination Delivery Unit only.\n",
    "IsExpressMessengerInternational\t|    BOOL\t|\tflag indicating if the facility handles international express messenger services.\n",
    "WindowsTimeZone\t|    STRING\t|\tTime zone used in Windows systems for the facility.\n",
    "ParentFacilityKey\t|    INT64\t|\tNumeric identifier for the parent facility, if applicable.\n",
    "BusinessUnit\t|    STRING\t|\tBusiness unit or segment associated with the facility.\n",
    "\n",
    "\n",
    "dim_date\n",
    "This table provides a date dimension with various attributes for calendar and fiscal periods. This table is essential for time-based analysis and reporting, offering indicators for holidays  and peak periods.\n",
    "column_name | data_type | description\n",
    "DateKey\t|    INT64\t|\tA unique identifier for each date\n",
    "Calendardate\t|    DATE\t|\tThe specific calenar date\n",
    "CalendarDatetime    |    DATETIME\t|\tThe timestamp of the date\n",
    "CalendarDayOfYear\t|    INT64\t|\tThe day of the year in the calendar\n",
    "CalendarMonthName\t|    STRING\t|\tThe name of the calendar month\n",
    "CalendarMonthNumber\t|    INT64\t|\tThe month the number in the calendar\n",
    "CalendarQuarter\t|    INT64\t|\tThe quarter of the calendar year\n",
    "CalendarDayOfWeekName\t|    STRING\t|\tThe name of the day of the week.\n",
    "CalendarDayOfWeek\t|    INT64\t|\tThe day of the week in the calendar\n",
    "CalendarDayOfMonth\t|    INT64\t|\tThe day of the month in the calendar\n",
    "CalendarWeekOfMonth\t|    INT64\t|\tThe week of the month in the calendar\n",
    "CalendarWeekOfYear\t|    INT64\t|\tThe week of the year in the calendar\n",
    "CalendarYear\t|    INT64\t|\tThe numeric year in the calendar\n",
    "CalendarYearMonthAsInteger\t|    INT64\t|\tA concatenation of the year and month as an integer.\n",
    "CalendarYearQuarterAsInteger\t|    INT64\t|\tA concatenation of the year and quarter as an integer.\n",
    "FirstDayOfCalendarMonthIndicator\t|    STRING\t|\tIndicator if the date is the first day of the calendar month.\n",
    "FirstdateOfCalendarMonth\t|    DATE\t|\tThe first date of the calendar month.\n",
    "LastDayOfCalendarMonthIndicator\t|    STRING\t|\tIndicator if the date is the last day of the calendar month.\n",
    "LastdateOfCalendarMonth\t|    DATE\t|\tThe last date of the calendar month.\n",
    "FirstdateOfCalendarWeek\t|    DATE\t|\tThe first date of the calendar week.\n",
    "LastdateOfCalendarWeek\t|    DATE\t|\tThe last date of the calendar week.\n",
    "HolidayIndicator\t|    STRING\t|\tIndicates if the date is a holiday\n",
    "HolidayName\t|    STRING\t|\tThe name of the holiday\n",
    "HolidayIndicatorObserved\t|    STRING\t|\tIndicator if the date is an observed holiday.\n",
    "HolidayNameObserved\t|    STRING\t|\tThe name of the observed holiday.\n",
    "WeekdayWeekend\t|    STRING\t|\tIndicates if the day is Weekday or Weekend\n",
    "VolumeTierEnddateIndicator\t|    STRING\t|\tIndicator if the date is the end of a volume tier.\n",
    "PeakIndicator\t|    STRING\t|\tIndicator if the date is during a peak period.\n",
    "Fiscaldate\t|    DATE\t|\tThe specific fiscal date\n",
    "FiscalDayOfYear\t|    INT64\t|\tThe day number within the fiscal year\n",
    "FiscalMonthName\t|    STRING\t|\tThe name of the fiscal month\n",
    "FiscalMonthNumber\t|    INT64\t|\tThe numeric representation of the fiscal month\n",
    "FiscalQuarter\t|    INT64\t|\tThe quarter of the fiscal year\n",
    "FiscalDayOfMonth\t|    INT64\t|\tThe numeric day within the fiscal month.\n",
    "FiscalYear\t|    INT64\t|\tThe fiscal year\n",
    "FiscalYearMonthAsInteger\t|    INT64\t|\tA concatenation of the fiscal year and month as an integer.\n",
    "FiscalYearQuarterAsInteger\t|    INT64\t|\tA concatenation of the fiscal year and quarter as an integer.\n",
    "FirstDayOfFiscalMonthIndicator\t|    STRING\t|\tIndicator if the date is the first day of the fiscal month.\n",
    "FirstdateOfFiscalMonth\t|    DATE\t|\tThe first date of the fiscal month.\n",
    "LastDayOfFiscalMonthIndicator\t|    STRING\t|\tIndicator if the date is the last day of the fiscal month.\n",
    "LastdateOfFiscalMonth\t|    DATE\t|\tThe last date of the fiscal month.\n",
    "FirstdateOfReportWeek\t|    DATE\t|\tThe first date of the report week.\n",
    "LastdateOfReportWeek\t|    DATE\t|\tThe last date of the report week.\n",
    "ReportDayOfWeekName\t|    STRING\t|\tThe name of the report day of the week.\n",
    "ReportDayOfWeek\t|    INT64\t|\tThe numeric representation of the report day of the week.\n",
    "ReportWeekOfYear\t|    INT64\t|\tThe week number within the report year.\n",
    "ReportWeekLabel\t|    STRING\t|\tA label for the report week.\n",
    "ReportYear\t|    INT64\t|\tThe report year\n",
    "ReportYearWeekAsInteger\t|    INT64\t|\tA concatenation of the report year and week as an integer.\n",
    "AlternativeReportWeekOfYear\t|    INT64\t|\tAn alternative week number within the report year.\n",
    "AlternativeReportWeekLabel\t|    STRING\t|\tAn alternative label for the report week.\n",
    "AlternativeReportYear\t|    INT64\t|\tAn alternative report year\n",
    "AlternativeReportYearWeekAsInteger\t|    INT64\t|\tA concatenation of the alternative report year and week as an integer.\n",
    "\n",
    "\n",
    "dim_customer\n",
    "This table holds comprehensive customer information, including customer names, industry types , and delivery preferences. The table also tracks billing schedules, signature requirements, and operational parameters.\n",
    "column_name | data_type | description\n",
    "CustomerName\t|    STRING\t|\tThe name of the customer\n",
    "Industry\t|    STRING\t|\tThe name of industry the customer belongs to\n",
    "ActiveFlag\t|    BOOL\t|\tIndicates if the customer is active.\n",
    "AttemptBillability\t|    INT64\t|\tThe count of billable attempts for the customer.\n",
    "AuditScanReport\t|    BOOL\t|\tIndicates if audit scan reporting is enabled for the customer.\n",
    "AuthorizedCheatingScans\t|    BOOL\t|\tIndicates if cheating scans are authorized for the customer.\n",
    "AuthorizedDeliveryMonday\t|    BOOL\t|\tIndicates if delivery is authorized on Mondays.\n",
    "AuthorizedDeliveryTuesday\t|    BOOL\t|\tIndicates if delivery is authorized on Tuesdays.\n",
    "AuthorizedDeliveryWednesday\t|    BOOL\t|\tIndicates if delivery is authorized on Wednesdays.\n",
    "AuthorizedDeliveryThursday\t|    BOOL\t|\tIndicates if delivery is authorized on Thursdays.\n",
    "AuthorizedDeliveryFriday\t|    BOOL\t|\tIndicates if delivery is authorized on Fridays.\n",
    "AuthorizedDeliverySaturday\t|    BOOL\t|\tIndicates if delivery is authorized on Saturdays.\n",
    "AuthorizedDeliverySunday\t|    BOOL\t|\tIndicates if delivery is authorized on Sundays.\n",
    "AuthorizedServices\t|    STRING\t|\tThe list of services authorized for the customer.\n",
    "BillingSchedule\t|    STRING\t|\tThe billing schedule for the customer.\n",
    "CalculatedDeliveryByMethod\t|    STRING\t|\tThe method used to calculate delivery time.\n",
    "DefaultSignatureType\t|    STRING\t|\tThe default type of signature required\n",
    "DefaultWeight\t|    INT64\t|\tThe default weight used for shipments\n",
    "DimensionFactor\t|    INT64\t|\tThe dimension factor used for shipping\n",
    "DimensionMinimumVolume\t|    INT64\t|\tThe minimum volume considered for dimensional pricing.\n",
    "GuesstimateTransitTime\t|    STRING\t|\tThe estimated transit time for shipments.\n",
    "InjectionPostalCode\t|    STRING\t|\tThe postal code where goods are injected into the system.\n",
    "ManifestForceSignatureRequired\t|    BOOL\t|\tIndicates if a signature is required on the manifest.\n",
    "ManifestReturnsAuthorized\t|    BOOL\t|\tIndicates if returns are authorized on the manifest.\n",
    "MaxAttempts\t|    INT64\t|\tThe maximum number of delivery attempts allowed\n",
    "MaxTransitDayForForward\t|    INT64\t|\tThe maximum number of transit days for forward delivery\n",
    "NextDayDeliveryCutOffTimeOther\t|    INT64\t|\tThe cut-off time for next-day delivery for other shipments.\n",
    "NextDayDeliveryCutOffTimeRegularDelivery\t|    INT64\t|\tThe cut-off time for next-day regular delivery.\n",
    "NextDayDeliveryCutOffTimeSamedayDelivery\t|    INT64\t|\tThe cut-off time for same-day delivery.\n",
    "ReuseBarcode\t|    BOOL\t|\tIndicated if the barcodes can be reused\n",
    "SignatureWaiveable\t|    BOOL\t|\tIndicates if the signature requirements can be waived\n",
    "UseCustomerDimensionForPricing\t|    BOOL\t|\tIndicates if the customer specific dimensions are used for pricing\n",
    "AccountDirector\t|    STRING\t|\tThe name of the account director managing the customer\n",
    "AccountOwner\t|    STRING\t|\tThe name of the account owner managing the customer\n",
    "AMTGroup\t|    STRING\t|\tThe account management team group associated with the customer.\n",
    "CustomerStart\t|    STRING\t|\tThe start daye of the customer relationship with the company\n",
    "FinanceTag\t|    STRING\t|\tA finance tag associated with the customer\n",
    "DefaultCustomerRollupCode\t|    STRING\t|\tThe default rollup customer code in reporting or billing systems.\n",
    "\n",
    "\n",
    "dim_service\n",
    "This table stores information about the various services offered, including service identifiers and descriptions. The table also includes legacy data and flags to determine how services are counted and billed.\n",
    "column_name | data_type | description\n",
    "ServiceId\t|    STRING\t|\tA unique identifier for the service.\n",
    "ServiceName\t|    STRING\t|\tThe name of the service.\n",
    "LegacyName\t|    STRING\t|\tThe name of the Legacy company of the service\n",
    "PickupOrDeliveryRevenue\t|    STRING\t|\tRevenue associated with pickup or delivery for the service.\n",
    "IsCountedAsPiece\t|    BOOL\t|\tIndicates if the service is counted as a separate piece.\n",
    "TransconNameId\t|    STRING\t|\tIdentifier for the name of the transcontinental service.\n",
    "TransconTypeId\t|    INT64\t|\tIdentifier for the type of transcontinental service.\n",
    "ServiceDefinition\t|    STRING\t|\tA description or definition of the service.\n",
    "LegacyCompany\t|    STRING\t|\tThe legacy company associated with the service.\n",
    "\n",
    "\n",
    "dim_account\n",
    "This table contains information about customer accounts, including account identifiers, account names, and statuses. It also tracks the relationship of accounts with customers and services, as well as categorization.\n",
    "column_name | data_type | description\n",
    "AccountId\t|    STRING\t|\tA unique identifier for the account.\n",
    "AardvarkId\t|    STRING\t|\tInternal ID used to distinguish certain accounts\n",
    "CustomerParentId\t|    STRING\t|\tThe identifier of the parent customer.\n",
    "FacilityCodeId\t|    STRING\t|\tThe identifier for the facility code associated with the account.\n",
    "ServiceCodeId\t|    STRING\t|\tThe identifier for the service code linked to the account.\n",
    "AccountName\t|    STRING\t|\tThe name of the account.\n",
    "AccountStatus\t|    INT64\t|\tThe status of the account, likely represented numerically.\n",
    "ParentAccount\t|    STRING\t|\tThe identifier of the parent account, if applicable.\n",
    "SubCustomerCode\t|    STRING\t|\tA code representing a sub-customer or sub-division of the account.\n",
    "RollupCode\t|    STRING\t|\tA code used for aggregating or \"rolling up\" accounts in reporting.\n",
    "RollupName\t|    STRING\t|\tThe name associated with the rollup code.\n",
    "LineOfBusCategory\t|    STRING\t|\tThe business category the account falls under.\n",
    "LineOfBusSubCategory\t|    STRING\t|\tThe subcategory within the business line.\n",
    "LegacyCompany\t|    STRING\t|\tThe legacy company (either LaserShip or Ontrac) associated with the account.\n",
    "\n",
    "\n",
    "Think step by step, then generate a consolidated SQL query that answers the user's query without any errors.\n",
    "\n",
    "Here are some examples for reference:\n",
    "Question:\n",
    "What are the top 5 performing customers this week\n",
    "\n",
    "\n",
    "SQL:\n",
    "```sql\n",
    "SELECT COUNT(*) AS NumVolume, CustomerName\n",
    "FROM onelook_test.fact_package fp\n",
    "\tJOIN onelook_test.dim_customer dc ON fp.CustomerKey = dc.CustomerKey\n",
    "WHERE ManifestDatetime >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\n",
    "GROUP BY CustomerName\n",
    "ORDER BY NumVolume DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "Question:\n",
    "What are the top 10 worst performing branches?\n",
    "\n",
    "SQL:\n",
    "```sql\n",
    "SELECT SUM(RevenueExclFuel - TotalDeliveryCost) AS Profit, BranchName\n",
    "FROM onelook_test.fact_package fp\n",
    "\tJOIN onelook_test.dim_branch db ON fp.DestinationBranchKey = db.BranchKey\n",
    "WHERE DeliveryDatetime >= DATE_SUB(CURRENT_DATE(), INTERVAL 40 DAY)\n",
    "\tAND  (hasManualDelivery = 1 OR hasScannedDelivery = 1) -- Make sure to only include delivered packages\n",
    "GROUP BY BranchName\n",
    "ORDER BY Profit ASC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "Question:\n",
    "What are the least utilized zips for Abercrombie (CustomerId CXO4)?\n",
    "\n",
    "SQL:\n",
    "```sql\n",
    "SELECT COUNT(*) AS NumVolume, DestinationZipCode\n",
    "FROM onelook_test.fact_package fp\n",
    "JOIN onelook_test.dim_customer dc ON fp.CustomerKey = dc.CustomerKey\n",
    "WHERE DeliveryDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 40 DAY)\n",
    "    AND dc.CustomerId = 'CXO4'\n",
    "GROUP BY DestinationZipCode\n",
    "ORDER BY NumVolume ASC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "Question:\n",
    "Trend of rate per piece for Abercrombie (CustomerId CXO4)?\n",
    "\n",
    "SQL:\n",
    "```sql\n",
    "SELECT AVG(RevenueExclFuel + Fuel) AS Rate, CalendarDate\n",
    "FROM onelook_test.fact_package fp\n",
    "\tJOIN onelook_test.dim_customer dc ON fp.CustomerKey = dc.CustomerKey\n",
    "WHERE DeliveryDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 40 DAY)\n",
    "\tAND CustomerId = 'CXO4'\n",
    "\tAND  (hasManualDelivery = 1 OR hasScannedDelivery = 1) -- Make sure to only include delivered packages\n",
    "GROUP BY DeliveryDate\n",
    "ORDER BY DeliveryDate ASC;\n",
    "```\n",
    "\n",
    "Question:\n",
    "Which facility had the highest number of damaged packages?\n",
    "\n",
    "SQL:\n",
    "```sql\n",
    "SELECT COUNT(*) AS DamageCount, f.FacilityName\n",
    "FROM onelook_test.fact_package fp\n",
    "JOIN onelook_test.dim_facility f ON fp.DamageFlagFacilityKey = f.FacilityKey\n",
    "WHERE fp.DamageFlag = TRUE\n",
    "GROUP BY f.FacilityName\n",
    "ORDER BY DamageCount DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "Question:\n",
    "Which facilities have the highest on-time delivery rates?\n",
    "\n",
    "SQL:\n",
    "```sql\n",
    "SELECT\n",
    "    f.FacilityName,\n",
    "    COUNT(DISTINCT CASE WHEN fp.isCustomerOntime THEN fp.BarcodeId ELSE NULL END) AS OnTimeDeliveries,\n",
    "    COUNT(DISTINCT fp.BarcodeId) AS TotalDeliveries,\n",
    "    (COUNT(DISTINCT CASE WHEN fp.isCustomerOntime THEN fp.BarcodeId ELSE NULL END) * 1.0 / COUNT(DISTINCT fp.BarcodeId)) * 100 AS OnTimeDeliveryRate\n",
    "FROM\n",
    "    `prj-ot-dev-bqsandbox-000001.onelook_test.fact_package` AS fp\n",
    "JOIN\n",
    "    `prj-ot-dev-bqsandbox-000001.onelook_test.dim_facility` AS f ON fp.DeliveryFacilityKey = f.FacilityKey\n",
    "GROUP BY\n",
    "    f.FacilityName\n",
    "ORDER BY\n",
    "    OnTimeDeliveryRate DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "Question:\n",
    "Which facility processed the most packages for the Temu?\n",
    "\n",
    "SQL:\n",
    "```\n",
    "SELECT\n",
    "    f.FacilityName,\n",
    "    COUNT(fp.BarcodeId) AS TotalPackagesProcessed\n",
    "FROM\n",
    "    `prj-ot-dev-bqsandbox-000001.onelook_test.fact_package` AS fp\n",
    "JOIN\n",
    "    `prj-ot-dev-bqsandbox-000001.onelook_test.dim_customer` AS c ON fp.CustomerKey = c.CustomerKey\n",
    "JOIN\n",
    "    `prj-ot-dev-bqsandbox-000001.onelook_test.dim_facility` AS f ON fp.DestinationFacilityKey = f.FacilityKey\n",
    "WHERE c.CustomerName LIKE '%Temu%'\n",
    "GROUP BY\n",
    "    f.FacilityName\n",
    "ORDER BY\n",
    "    TotalPackagesProcessed DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "{context_text}\n",
    "\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = get_prompt_for_gemini(query, final_context_text)\n",
    "\n",
    "\n",
    "# Generate the SQL query\n",
    "response = get_gemini_response(\n",
    "        multimodal_model,\n",
    "        model_input=[prompt],\n",
    "        stream=True,\n",
    "        generation_config=GenerationConfig(temperature=0, top_p = 0, max_output_tokens=2048)  \n",
    "    )\n",
    "\n",
    "# Ensure the response is trimmed of any unexpected whitespace\n",
    "sql_query = response.strip()\n",
    "\n",
    "# Extract the SQL query using regular expression\n",
    "match = re.search(r\"```sql\\n(.*)\\n```\", sql_query, flags=re.DOTALL)\n",
    "if match:\n",
    "        extracted_sql = match.group(1)\n",
    "        print(\"Generated SQL Query:\\n\", extracted_sql)\n",
    "else:\n",
    "        print(\"No SQL query found in the response.\")\n",
    "        \n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(project='prj-ot-dev-bqsandbox-000001')\n",
    "\n",
    "print('\\n')\n",
    "# Execute the SQL query\n",
    "try:\n",
    "      response_query_job = client.query(extracted_sql)\n",
    "      df = response_query_job.to_dataframe()\n",
    "      print(df)\n",
    "      # Update conversation history with results\n",
    "      update_conversation_history(query, extracted_sql, df.to_dict())\n",
    "except Exception as e:\n",
    "      print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Handle follow-up queries\n",
    "while True:\n",
    "     follow_up_query = input(\"Would you like to ask a follow-up question? (yes/no) \")\n",
    "     if follow_up_query.lower() == 'no':\n",
    "         break\n",
    "\n",
    " # Process follow-up query\n",
    "     follow_up_question = input(\"Enter your follow-up question: \")\n",
    "     # Use previous results if needed\n",
    "     context_text = \"\\n\".join([f\"Previous Question: {entry['question']}\\nSQL Query: {entry['sql_query']}\\nResults: {entry.get('results', 'No results')}\" for entry in conversation_history])\n",
    "     prompt = get_prompt_for_gemini(follow_up_question, context_text)\n",
    "\n",
    "     response = get_gemini_response(\n",
    "          multimodal_model,\n",
    "          model_input=[prompt],\n",
    "          stream=True,\n",
    "          generation_config=None  # Replace with actual configuration\n",
    "     )\n",
    "\n",
    "     # Ensure the response is trimmed of any unexpected whitespace\n",
    "     sql_query = response.strip()\n",
    "\n",
    "     # Extract the SQL query using regular expression\n",
    "     match = re.search(r\"```sql\\n(.*)\\n```\", sql_query, flags=re.DOTALL)\n",
    "     if match:\n",
    "            extracted_sql = match.group(1)\n",
    "            print(\"Generated SQL Query:\\n\", extracted_sql)\n",
    "     else:\n",
    "           print(\"No SQL query found in the response.\")\n",
    "           continue\n",
    "\n",
    "     # Execute the follow-up SQL query\n",
    "     try:\n",
    "            response_query_job = client.query(extracted_sql)\n",
    "            df = response_query_job.to_dataframe()\n",
    "            print(df)\n",
    "            # Update conversation history with results\n",
    "            update_conversation_history(follow_up_question, extracted_sql, df.to_dict())\n",
    "     except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")    \n",
    "\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
